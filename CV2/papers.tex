\section{Publications}
\begin{enumerate}
\item
\textbf{\href{https://link.springer.com/article/10.1007/s10287-023-00493-9}{Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters}} (Olga Yufereva, Michael Persiianov, Pavel Dvurechensky, Alexander Gasnikov, Dmitry Kovalev). \textit{Computational Management Science}, 2024.
\item
\textbf{\href{https://link.springer.com/article/10.1007/s10287-023-00485-9}{Decentralized saddle-point problems with different constants of strong convexity and strong concavity}} (Dmitry Metelev, Alexander Rogozin, Alexander Gasnikov, Dmitry Kovalev). \textit{Computational Management Science}, 2024.
\item
\textbf{\href{https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062}{Decentralized saddle point problems via non-Euclidean mirror prox}} (Alexander Rogozin, Aleksandr Beznosikov, Darina Dvinskikh, Dmitry Kovalev, Pavel Dvurechensky, Alexander Gasnikov). \textit{Optimization Methods and Software}, 2024.
\item
\textbf{\href{https://link.springer.com/article/10.1007/s10287-023-00479-7}{Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs}} (Aleksandr Lobanov, Andrew Veprikov, Georgiy Konin, Aleksandr Beznosikov, Alexander Gasnikov, Dmitry Kovalev). \textit{Computational Management Science}, 2023.
\item
\textbf{\href{https://link.springer.com/content/pdf/10.1007/978-3-030-54621-2_860-1.pdf}{Decentralized Convex Optimization over Time-Varying Graphs}} (Alexander Rogozin, Alexander Gasnikov, Aleksander Beznosikov, Dmitry Kovalev). \textit{Encyclopedia of Optimization}, 2023.
\item
\textbf{\href{https://ems.press/journals/mag/articles/9939904}{Smooth monotone stochastic variational inequalities and saddle point problems: A survey}} (Aleksandr Beznosikov, Boris Polyak, Eduard Gorbunov, Dmitry Kovalev, Alexander Gasnikov). \textit{European Mathematical Society Magazine}, 2023.
\item
\textbf{\href{https://proceedings.mlr.press/v202/metelev23a.html}{Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?}} (Dmitry Metelev, Alexander Rogozin, Dmitry Kovalev, Alexander Gasnikov). \textit{International Conference on Machine Learning}, 2023.
\item
\textbf{\href{https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355}{Stochastic distributed learning with gradient quantization and double-variance reduction}} (Samuel Horvath, Dmitry Kovalev, Konstantin Mishchenko, Peter Richtarik, Sebastian Stich). \textit{Optimization Methods and Software}, 2023.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html}{Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling}} (Dmitry Kovalev, Alexander Gasnikov, Peter Richtarik). \textit{Advances in Neural Information Processing Systems}, 2022.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html}{Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox}} (Abdurakhmon Sadiev, Dmitry Kovalev, Peter Richtarik). \textit{Advances in Neural Information Processing Systems}, 2022.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html}{Optimal algorithms for decentralized stochastic variational inequalities}} (Dmitry Kovalev, Aleksandr Beznosikov, Abdurakhmon Sadiev, Michael Persiianov, Peter Richtarik, Alexander Gasnikov). \textit{Advances in Neural Information Processing Systems}, 2022.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html}{Optimal gradient sliding and its application to optimal distributed optimization under similarity}} (Dmitry Kovalev, Aleksandr Beznosikov, Ekaterina Borodich, Alexander Gasnikov, Gesualdo Scutari). \textit{Advances in Neural Information Processing Systems}, 2022.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html}{The first optimal acceleration of high-order methods in smooth convex optimization}} (Dmitry Kovalev, Alexander Gasnikov). \textit{Advances in Neural Information Processing Systems}, 2022.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html}{The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization}} (Dmitry Kovalev, Alexander Gasnikov). \textit{Advances in Neural Information Processing Systems}, 2022.
\item
\textbf{\href{http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf}{Accelerated variance-reduced methods for saddle-point problems}} (Ekaterina Borodich, Vladislav Tominin, Yaroslav Tominin, Dmitry Kovalev, Alexander Gasnikov, Pavel Dvurechensky). \textit{EURO Journal on Computational Optimization}, 2022.
\item
\textbf{\href{https://proceedings.mlr.press/v151/salim22a.html}{An optimal algorithm for strongly convex minimization under affine constraints}} (Adil Salim, Laurent Condat, Dmitry Kovalev, Peter Richtarik). \textit{International Conference on Artificial Intelligence and Statistics}, 2022.
\item
\textbf{IntSGD: Adaptive floatless compression of stochastic gradients} (Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, Peter Richtarik). \textit{International Conference on Learning Representations}, 2022.
\item
\textbf{\href{https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html}{Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks}} (Dmitry Kovalev, Elnur Gasanov, Alexander Gasnikov, Peter Richtarik). \textit{Advances in Neural Information Processing Systems}, 2021.
\item
\textbf{\href{https://proceedings.mlr.press/v130/kovalev21a.html}{A linearly convergent algorithm for decentralized optimization: Sending less bits for free!}} (Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, Sebastian Stich). \textit{International Conference on Artificial Intelligence and Statistics}, 2021.
\item
\textbf{\href{http://proceedings.mlr.press/v139/kovalev21a}{ADOM: accelerated decentralized optimization method for time-varying networks}} (Dmitry Kovalev, Egor Shulgin, Peter Richtarik, Alexander V Rogozin, Alexander Gasnikov). \textit{International Conference on Machine Learning}, 2021.
\item
\textbf{\href{https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18}{Near-optimal decentralized algorithms for saddle point problems over time-varying networks}} (Aleksandr Beznosikov, Alexander Rogozin, Dmitry Kovalev, Alexander Gasnikov). \textit{Optimization and Applications: 12th International Conference, OPTIMA 2021, Petrovac, Montenegro, September 27–October 1, 2021, Proceedings 12}, 2021.
\item
\textbf{\href{https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19}{Towards accelerated rates for distributed optimization over time-varying networks}} (Alexander Rogozin, Vladislav Lukoshkin, Alexander Gasnikov, Dmitry Kovalev, Egor Shulgin). \textit{Optimization and Applications: 12th International Conference, OPTIMA 2021, Petrovac, Montenegro, September 27–October 1, 2021, Proceedings 12}, 2021.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html}{Linearly converging error compensated SGD}} (Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, Peter Richtarik). \textit{Advances in Neural Information Processing Systems}, 2020.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html}{Optimal and practical algorithms for smooth and strongly convex decentralized optimization}} (Dmitry Kovalev, Adil Salim, Peter Richtarik). \textit{Advances in Neural Information Processing Systems}, 2020.
\item
\textbf{\href{https://proceedings.mlr.press/v117/kovalev20a.html}{Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop}} (Dmitry Kovalev, Samuel Horvath, Peter Richtarik). \textit{Algorithmic Learning Theory}, 2020.
\item
\textbf{\href{https://link.springer.com/article/10.1134/S0965542520110020}{Accelerated methods for saddle-point problem}} (Mohammad S Alkousa, Alexander Vladimirovich Gasnikov, Darina Mikhailovna Dvinskikh, Dmitry A Kovalev, Fedor Sergeevich Stonyakin). \textit{Computational Mathematics and Mathematical Physics}, 2020.
\item
\textbf{\href{http://proceedings.mlr.press/v108/mishchenko20a}{Revisiting stochastic extragradient}} (Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richtarik, Yura Malitsky). \textit{International Conference on Artificial Intelligence and Statistics}, 2020.
\item
\textbf{\href{https://proceedings.mlr.press/v119/li20g.html}{Acceleration for compressed gradient descent in distributed and federated optimization}} (Zhize Li, Dmitry Kovalev, Xun Qian, Peter Richtarik). \textit{International Conference on Machine Learning}, 2020.
\item
\textbf{\href{http://proceedings.mlr.press/v119/malinovskiy20a.html}{From local SGD to local fixed-point methods for federated learning}} (Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, Peter Richtarik). \textit{International Conference on Machine Learning}, 2020.
\item
\textbf{\href{https://proceedings.mlr.press/v119/hanzely20b.html}{Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems}} (Filip Hanzely, Dmitry Kovalev, Peter Richtarik). \textit{International Conference on Machine Learning}, 2020.
\item
\textbf{\href{https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html}{RSN: randomized subspace Newton}} (Robert Gower, Dmitry Kovalev, Felix Lieder, Peter Richtarik). \textit{Advances in Neural Information Processing Systems}, 2019.
\item
\textbf{\href{https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html}{Stochastic proximal langevin algorithm: Potential splitting and nonasymptotic rates}} (Adil Salim, Dmitry Kovalev, Peter Richtarik). \textit{Advances in Neural Information Processing Systems}, 2019.
\item
\textbf{\href{https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html}{Stochastic spectral and conjugate descent methods}} (Dmitry Kovalev, Peter Richtarik, Eduard Gorbunov, Elnur Gasanov). \textit{Advances in Neural Information Processing Systems}, 2018.
\item
\textbf{\href{http://crm-en.ics.org.ru/journal/article/2685/}{A hypothesis about the rate of global convergence for optimal methods (Newton’s type) in smooth convex optimization}} (Alexander Gasnikov, Dmitry Kovalev). \textit{Computer research and modeling}, 2018.
\end{enumerate}
\section{Preprints}
\begin{enumerate}
\item
\textbf{On Linear Convergence in Smooth Convex-Concave Bilinearly-Coupled Saddle-Point Optimization: Lower Bounds and Optimal Algorithms} (Dmitry Kovalev, Ekaterina Borodich). \textit{arXiv preprint arXiv:2411.14601}, 2024.
\item
\textbf{Decentralized Optimization with Coupled Constraints} (Demyan Yarmoshik, Alexander Rogozin, Nikita Kiselev, Daniil Dorin, Alexander Gasnikov, Dmitry Kovalev). \textit{arXiv preprint arXiv:2407.02020}, 2024.
\item
\textbf{Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks} (Dmitry Kovalev, Ekaterina Borodich, Alexander Gasnikov, Dmitrii Feoktistov). \textit{arXiv preprint arXiv:2405.18031}, 2024.
\item
\textbf{Decentralized finite-sum optimization over time-varying networks} (Dmitry Metelev, Savelii Chezhegov, Alexander Rogozin, Aleksandr Beznosikov, Alexander Sholokhov, Alexander Gasnikov, Dmitry Kovalev). \textit{arXiv preprint arXiv:2402.02490}, 2024.
\item
\textbf{Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems} (Ekaterina Borodich, Georgiy Kormakov, Dmitry Kovalev, Aleksandr Beznosikov, Alexander Gasnikov). \textit{arXiv preprint arXiv:2307.12946}, 2023.
\item
\textbf{An optimal algorithm for strongly convex min-min optimization} (Alexander Gasnikov, Dmitry Kovalev, Grigory Malinovsky). \textit{arXiv preprint arXiv:2212.14439}, 2022.
\item
\textbf{On scaled methods for saddle point problems} (Aleksandr Beznosikov, Aibek Alanov, Dmitry Kovalev, Martin Takac, Alexander Gasnikov). \textit{arXiv preprint arXiv:2206.08303}, 2022.
\item
\textbf{Decentralized distributed optimization for saddle point problems} (Alexander Rogozin, Aleksandr Beznosikov, Darina Dvinskikh, Dmitry Kovalev, Pavel Dvurechensky, Alexander Gasnikov). \textit{arXiv preprint arXiv:2102.07758}, 2021.
\item
\textbf{Fast linear convergence of randomized BFGS} (Dmitry Kovalev, Robert M Gower, Peter Richtarik, Alexander Rogozin). \textit{arXiv preprint arXiv:2002.11337}, 2020.
\item
\textbf{Distributed fixed point methods with compressed iterates} (Selim Chraibi, Ahmed Khaled, Dmitry Kovalev, Peter Richtarik, Adil Salim, Martin Takac). \textit{arXiv preprint arXiv:1912.09925}, 2019.
\item
\textbf{Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates} (Dmitry Kovalev, Konstantin Mishchenko, Peter Richtarik). \textit{arXiv preprint arXiv:1912.01597}, 2019.
\end{enumerate}
