\section{Conference Posters and Talks}
\begin{enumerate}
  \item {\bf Talk: Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization}, {\em Traditional School (Control, Information and Optimization)}, Innopolis University, Kazan, Russia (June 2025)
  \item {\bf Talk: Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization}, {\em Data Fusion 2025 Conference}, Moscow, Russia (April 2025)
  \item {\bf Talk: Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks}, {\em Rising Stars in AI Symposium 2022}, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia (March 2022)
  \item {\bf Talk/Poster: Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks}, {\em NeurIPS 2021}, Online (December 2021)
  \item {\bf Talk/Poster: Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks}, {\em International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2021}, Online (July 2021)
  \item {\bf Talk/Poster: ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks}, {\em ICML 2021}, Online (July 2021)
  \item {\bf Poster: ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks}, {\em Optimization Without Borders Conference}, Sirius University, Sochi, Russia (July 2021)
  \item {\bf Talk/Poster: A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!}, {\em AISTATS 2021}, Online (April 2021)
  \item {\bf Poster: Linearly Converging Error Compensated SGD}, {\em NeurIPS 2020}, Online (December 2020)
  \item {\bf Talk/Poster: Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization}, {\em NeurIPS 2020}, Online (December 2020)
  \item {\bf Talk: Variance Reduced Coordinate Descent with Acceleration: New Method With a Surprising Application to Finite-Sum Problems}, {\em ICML 2020}, Online (July 2020)
  \item {\bf Poster: RSN: Randomized Subspace Newton}, {\em NeurIPS 2019}, Vancouver, Canada (December 2019)
  \item {\bf Poster: Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates}, {\em NeurIPS 2019}, Vancouver, Canada (December 2019)
  \item {\bf Talk: Revisiting Stochastic Extragradient Method}, {\em International Conference on Continuous Optimization 2019}, Technical University, Berlin, Germany (August 2019)
  \item {\bf Poster: Stochastic Distributed Learning with Gradient Quantization and Variance Reduction}, {\em Data Science Summer School 2019}, Ecole Polytechnique, Paris, France (June 2019)
  \item {\bf Poster: Stochastic Distributed Learning with Gradient Quantization and Variance Reduction}, {\em Traditional School (Control, Information and Optimization)}, Higher School of Economics Study Center, Voronovo, Russia (June 2019)
  \item {\bf Talk: Stochastic Spectral Descent Methods}, {\em Automatic control and Optimization Theory Weekly Seminar}, Institute for Control Problems, Moscow, Russia (March 2019)
  \item {\bf Talk: Stochastic Distributed Learning with Gradient Quantization and Variance Reduction}, {\em Modern Optimization Methods Seminar}, Moscow Institute of Physics and Technology, Moscow, Russia (March 2019)
  \item {\bf Poster: Stochastic Spectral Descent Methods}, {\em NeurIPS 2018}, Montreal, Canada (December 2018)
  \item {\bf Poster: Stochastic Spectral Descent Methods}, {\em Optimization and Big Data Workshop}, King Abdullah University of Science and Technology, Thuwal, Saudi Arabia (June 2018)
  \item {\bf Poster: Stochastic Spectral Descent Methods}, {\em Traditional School (Control, Information and Optimization)}, Higher School of Economics Study Center, Voronovo, Russia (February 2018)
\end{enumerate}
