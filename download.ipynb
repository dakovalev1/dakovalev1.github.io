{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import typing\n",
    "import bs4\n",
    "import pickle\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "PROXY_URL = \"http://5zipXAuZVPsquwtL:wifi;;;;@proxy.froxy.com:9000\"\n",
    "SCHOLAR_PROFILE = \"qHFA5z4AAAAJ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(url: str) -> str:\n",
    "    with requests.request(\n",
    "        method=\"GET\",\n",
    "        url=url,\n",
    "        headers={\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15\",\n",
    "            # \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\",\n",
    "        },\n",
    "        proxies={\"https\": PROXY_URL, \"http\": PROXY_URL},\n",
    "    ) as response:\n",
    "\n",
    "        print(\"url =\", url, \"status =\", response.status_code)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://scholar.google.com/citations?hl=en&user=qHFA5z4AAAAJ&pagesize=100 status = 200\n",
      "51\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Y0pCki6q_DkC title = Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:2osOgNQ5qMEC title = Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:bnK-pcrLprsC title = Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Se3iqnhoufwC title = From local SGD to local fixed-point methods for federated learning\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:IjCSPb-OGe4C title = RSN: randomized subspace Newton\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tyk-4Ss8FVUC title = Revisiting stochastic extragradient\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:8k81kl-MbHgC title = Linearly converging error compensated SGD\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:UebtZRa9Y70C title = Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:MXK_kJrjxJIC title = A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:kNdYIx-mwKoC title = Accelerated methods for saddle-point problem\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:YsMSGLbcyi4C title = Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RHpTSmoSYBkC title = Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:9ZlFYXVOiuMC title = Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:KlAtU1dfN6UC title = Decentralized distributed optimization for saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:GnPB-g6toBAC title = The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ldfaerwXgEUC title = Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RGFaLdJalmkC title = Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ULOm3_A8WrAC title = ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:TFP_iSt0sucC title = Optimal algorithms for decentralized stochastic variational inequalities\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:HoB7MX3m0LUC title = Accelerated variance-reduced methods for saddle-point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:nb7KW1ujOQ8C title = IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4TOpqqG69KYC title = An optimal algorithm for strongly convex minimization under affine constraints\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:0EnyYjriUFMC title = Towards accelerated rates for distributed optimization over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:zYLM7Y9cAGgC title = Stochastic proximal Langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:BqipwSGYUEgC title = The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:35N4QoGY0k4C title = Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:eQOLeE2rZwMC title = Distributed fixed point methods with compressed iterates\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ufrVoPGSRksC title = Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:LkGwnXOMwfcC title = Fast linear convergence of randomized BFGS\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:7PzlFSSx8tAC title = Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:NaGl4SEjCO4C title = Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:dshw04ExmUIC title = Decentralized convex optimization over time-varying graphs\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:qjMakFHDy7sC title = Stochastic spectral and conjugate descent methods\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:_xSYboBqXhAC title = Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:J_g5lzvAfSwC title = On scaled methods for saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Z5m8FVwuT1cC title = Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4OULZ7Gr8RgC title = An Optimal Algorithm for Strongly Convex Min-Min Optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pyW8ca7W8N0C title = Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:d1gkVwhDpl0C title = A hypothesis about the rate of global convergence for optimal methods (Newtonâ€™s type) in smooth convex optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:p2g8aNsByqUC title = Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:EUQCXRtRnyEC title = Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:yD5IFk8b50cC title = Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:M7yex6snE4oC title = On Linear Convergence in Smooth Convex-Concave Bilinearly-Coupled Saddle-Point Optimization: Lower Bounds and Optimal Algorithms\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tiz5es2fbqcC title = Decentralized finite-sum optimization over time-varying networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:geHnlv5EZngC title = Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:oNZyr7d5Mn4C title = SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:epqYDVWIO7EC title = On Solving Minimization and Min-Max Problems by First-Order Methods with Relative Error in Gradients\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:WZBGuue-350C title = Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:V3AGJWp-ZtQC title = Decentralized Optimization with Coupled Constraints\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:tzM49s52ZIMC title = Convex-Concave Interpolation and Application of PEP to the Bilinear-Coupled Saddle Point Problem\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pqnbT2bcN3wC title = Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n"
     ]
    }
   ],
   "source": [
    "html = get(\n",
    "    f\"https://scholar.google.com/citations?hl=en&user={SCHOLAR_PROFILE}&pagesize=100\"\n",
    ")\n",
    "soup = bs4.BeautifulSoup(html)\n",
    "paper_list: bs4.ResultSet[bs4.Tag] = soup.find_all(\n",
    "    name=\"tr\", attrs={\"class\": \"gsc_a_tr\"}\n",
    ")\n",
    "\n",
    "print(len(paper_list))\n",
    "for paper in paper_list:\n",
    "    paper_link = paper.find(name=\"a\", attrs={\"class\": \"gsc_a_at\"})\n",
    "    print(\n",
    "        \"url =\",\n",
    "        \"https://scholar.google.com\" + paper_link.attrs[\"href\"],\n",
    "        \"title =\",\n",
    "        paper_link.text,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get paper cards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paper_soup2dict(paper_html: str) -> typing.Dict[str, bs4.Tag]:\n",
    "    paper_soup = bs4.BeautifulSoup(paper_html)\n",
    "\n",
    "    key_list: bs4.ResultSet[bs4.Tag] = paper_soup.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_field\"}\n",
    "    )\n",
    "    value_list: bs4.ResultSet[bs4.Tag] = paper_soup.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_value\"}\n",
    "    )\n",
    "\n",
    "    paper_dict: typing.Dict[str, bs4.Tag] = {}\n",
    "    paper_dict[\"Title\"] = paper_soup.find(name=\"div\", attrs={\"id\": \"gsc_oci_title\"})\n",
    "    for key, value in zip(key_list, value_list):\n",
    "        paper_dict[key.text] = value\n",
    "\n",
    "    return paper_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Y0pCki6q_DkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:2osOgNQ5qMEC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:bnK-pcrLprsC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Se3iqnhoufwC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:IjCSPb-OGe4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tyk-4Ss8FVUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:8k81kl-MbHgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:UebtZRa9Y70C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:MXK_kJrjxJIC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:kNdYIx-mwKoC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:YsMSGLbcyi4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RHpTSmoSYBkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:9ZlFYXVOiuMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:KlAtU1dfN6UC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:GnPB-g6toBAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ldfaerwXgEUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:RGFaLdJalmkC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ULOm3_A8WrAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:TFP_iSt0sucC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:HoB7MX3m0LUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:nb7KW1ujOQ8C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4TOpqqG69KYC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:0EnyYjriUFMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:zYLM7Y9cAGgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:BqipwSGYUEgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:35N4QoGY0k4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:eQOLeE2rZwMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:ufrVoPGSRksC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:LkGwnXOMwfcC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:7PzlFSSx8tAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:NaGl4SEjCO4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:dshw04ExmUIC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:qjMakFHDy7sC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:_xSYboBqXhAC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:J_g5lzvAfSwC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Z5m8FVwuT1cC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:4OULZ7Gr8RgC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pyW8ca7W8N0C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:d1gkVwhDpl0C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:p2g8aNsByqUC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:EUQCXRtRnyEC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:yD5IFk8b50cC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:M7yex6snE4oC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:Tiz5es2fbqcC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:geHnlv5EZngC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:oNZyr7d5Mn4C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:epqYDVWIO7EC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:WZBGuue-350C status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:V3AGJWp-ZtQC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:tzM49s52ZIMC status = 200\n",
      "url = https://scholar.google.com/citations?view_op=view_citation&hl=en&user=qHFA5z4AAAAJ&pagesize=100&citation_for_view=qHFA5z4AAAAJ:pqnbT2bcN3wC status = 200\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = []\n",
    "for paper in paper_list:\n",
    "    paper_url = (\n",
    "        \"https://scholar.google.com\"\n",
    "        + paper.find(name=\"a\", attrs={\"class\": \"gsc_a_at\"}).attrs[\"href\"]\n",
    "    )\n",
    "    paper_html = get(paper_url)\n",
    "    paper_dict = paper_soup2dict(paper_html)\n",
    "    paper_dict_list.append(paper_dict)\n",
    "pickle.dump(paper_dict_list, open(\"papers.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get paper versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Title', 'Authors', 'Publication date', 'Journal', 'Volume', 'Issue', 'Pages', 'Publisher', 'Description', 'Total citations', 'Scholar articles'])\n",
      "0 Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "1 Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "2 Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "3 From local SGD to local fixed-point methods for federated learning\n",
      "4 RSN: randomized subspace Newton\n",
      "5 Revisiting stochastic extragradient\n",
      "6 Linearly converging error compensated SGD\n",
      "7 Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "8 A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "9 Accelerated methods for saddle-point problem\n",
      "10 Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "11 Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "12 Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "13 Decentralized distributed optimization for saddle point problems\n",
      "14 The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "15 Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "16 Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "17 ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "18 Optimal algorithms for decentralized stochastic variational inequalities\n",
      "19 Accelerated variance-reduced methods for saddle-point problems\n",
      "20 IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "21 An optimal algorithm for strongly convex minimization under affine constraints\n",
      "22 Towards accelerated rates for distributed optimization over time-varying networks\n",
      "23 Stochastic proximal Langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "24 The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "25 Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "26 Distributed fixed point methods with compressed iterates\n",
      "27 Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "28 Fast linear convergence of randomized BFGS\n",
      "29 Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "30 Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "31 Decentralized convex optimization over time-varying graphs\n",
      "32 Stochastic spectral and conjugate descent methods\n",
      "33 Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "34 On scaled methods for saddle point problems\n",
      "35 Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization\n",
      "36 An Optimal Algorithm for Strongly Convex Min-Min Optimization\n",
      "37 Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "38 A hypothesis about the rate of global convergence for optimal methods (Newtonâ€™s type) in smooth convex optimization\n",
      "39 Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "40 Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "41 Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "42 On Linear Convergence in Smooth Convex-Concave Bilinearly-Coupled Saddle-Point Optimization: Lower Bounds and Optimal Algorithms\n",
      "43 Decentralized finite-sum optimization over time-varying networks\n",
      "44 Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "45 SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration\n",
      "46 On Solving Minimization and Min-Max Problems by First-Order Methods with Relative Error in Gradients\n",
      "47 Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization\n",
      "48 Decentralized Optimization with Coupled Constraints\n",
      "49 Convex-Concave Interpolation and Application of PEP to the Bilinear-Coupled Saddle Point Problem\n",
      "50 Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = pickle.load(\n",
    "    open(\"papers.txt\", \"rb\")\n",
    ")\n",
    "print(paper_dict_list[0].keys())\n",
    "for i in range(len(paper_dict_list)):\n",
    "    print(i, paper_dict_list[i][\"Title\"].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_version_urls(scholar_articles: bs4.Tag) -> typing.List[str]:\n",
    "    merged_snippet_list: bs4.ResultSet[bs4.Tag] = scholar_articles.find_all(\n",
    "        name=\"div\", attrs={\"class\": \"gsc_oci_merged_snippet\"}\n",
    "    )\n",
    "\n",
    "    version_url_list: typing.List[str] = []\n",
    "    for merged_snippet in merged_snippet_list:\n",
    "        version_href = merged_snippet.find(name=\"a\").attrs[\"href\"]\n",
    "        version_cluster = urllib.parse.parse_qs(\n",
    "            urllib.parse.urlparse(version_href).query\n",
    "        )[\"cluster\"][0]\n",
    "        version_url = f\"https://scholar.google.com/scholar?start=0&num=100&hl=en&cluster={version_cluster}\"\n",
    "        version_url_list.append(version_url)\n",
    "    return version_url_list\n",
    "\n",
    "\n",
    "def extract_versions(scholar_articles: bs4.Tag) -> typing.List[str]:\n",
    "    url_list = extract_version_urls(scholar_articles)\n",
    "\n",
    "    version_list: typing.List[str] = []\n",
    "    for url in url_list:\n",
    "        soup = bs4.BeautifulSoup(get(url))\n",
    "        versions: bs4.ResultSet[bs4.Tag] = soup.find_all(attrs={\"class\": \"gs_rt\"})\n",
    "        for version in versions:\n",
    "            a = version.find(name=\"a\")\n",
    "            if not a is None:\n",
    "                version_list.append(a.attrs[\"href\"])\n",
    "    return version_list\n",
    "\n",
    "\n",
    "# extract_versions(paper_dict_list[9][\"Scholar articles\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=10432066948921138844 status = 200\n",
      "Donâ€™t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=7006814588298832736 status = 200\n",
      "Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=1371688885190462102 status = 200\n",
      "From local SGD to local fixed-point methods for federated learning\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=15088076385024246021 status = 200\n",
      "RSN: randomized subspace Newton\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=7373789986788030402 status = 200\n",
      "Revisiting stochastic extragradient\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=8653605232017703121 status = 200\n",
      "Linearly converging error compensated SGD\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=9254067822190880000 status = 200\n",
      "Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=15665175903150090179 status = 200\n",
      "A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=3790088210576421356 status = 200\n",
      "Accelerated methods for saddle-point problem\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=9373209880233203502 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=4686524615161672519 status = 200\n",
      "Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=3002705566416746554 status = 200\n",
      "Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=313086167555561841 status = 200\n",
      "Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=218386955491957145 status = 200\n",
      "Decentralized distributed optimization for saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=15155949253390165076 status = 200\n",
      "The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=12882769054337015160 status = 200\n",
      "Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=6416508855679529689 status = 200\n",
      "Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=15617746434035789497 status = 200\n",
      "ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=13055424477930940340 status = 200\n",
      "Optimal algorithms for decentralized stochastic variational inequalities\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=16974920721308888306 status = 200\n",
      "Accelerated variance-reduced methods for saddle-point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=12130567569437016761 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=14409392578980627652 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=9962708434638871334 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=1257530239269417616 status = 200\n",
      "IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=16969044896100418296 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=15707566150174104807 status = 200\n",
      "An optimal algorithm for strongly convex minimization under affine constraints\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=15110157264304090339 status = 200\n",
      "Towards accelerated rates for distributed optimization over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=2324971111724151487 status = 200\n",
      "Stochastic proximal Langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=8964049524700423512 status = 200\n",
      "The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=6878778451294198426 status = 200\n",
      "Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=10468598298336511368 status = 200\n",
      "Distributed fixed point methods with compressed iterates\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=9469053425606975651 status = 200\n",
      "Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=14069246331553141070 status = 200\n",
      "Fast linear convergence of randomized BFGS\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=12043257693440176889 status = 200\n",
      "Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=4252407442233130977 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=11682401705022143728 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=18367348657283013424 status = 200\n",
      "Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=9024856644395226101 status = 200\n",
      "Decentralized convex optimization over time-varying graphs\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=14256695056462406414 status = 200\n",
      "Stochastic spectral and conjugate descent methods\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=15120261814030278283 status = 200\n",
      "Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=3502687140939495832 status = 200\n",
      "On scaled methods for saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=6878525993176263563 status = 200\n",
      "Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=9464268600160663021 status = 200\n",
      "An Optimal Algorithm for Strongly Convex Min-Min Optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=13028606441871676013 status = 200\n",
      "Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=9454741829138024660 status = 200\n",
      "A hypothesis about the rate of global convergence for optimal methods (Newtonâ€™s type) in smooth convex optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=3812646967675013786 status = 200\n",
      "Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=10357839628968722372 status = 200\n",
      "Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=3365804316294644363 status = 200\n",
      "Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=3040641654477328482 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=4942379545739387070 status = 200\n",
      "On Linear Convergence in Smooth Convex-Concave Bilinearly-Coupled Saddle-Point Optimization: Lower Bounds and Optimal Algorithms\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=6229982026178002764 status = 200\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=1214191133118673585 status = 200\n",
      "Decentralized finite-sum optimization over time-varying networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=9027699018424793418 status = 200\n",
      "Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=11605182395180550612 status = 200\n",
      "SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=658499838430938452 status = 200\n",
      "On Solving Minimization and Min-Max Problems by First-Order Methods with Relative Error in Gradients\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=10910203543962959848 status = 200\n",
      "Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=7506260031426519626 status = 200\n",
      "Decentralized Optimization with Coupled Constraints\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=13797128321053821676 status = 200\n",
      "Convex-Concave Interpolation and Application of PEP to the Bilinear-Coupled Saddle Point Problem\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=8894740852949274823 status = 200\n",
      "Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n",
      "url = https://scholar.google.com/scholar?start=0&num=100&hl=ru&cluster=7226585304015966991 status = 200\n"
     ]
    }
   ],
   "source": [
    "for paper_dict in paper_dict_list:\n",
    "    print(paper_dict[\"Title\"].text)\n",
    "    if \"Scholar articles\" in paper_dict.keys():\n",
    "        version_list = extract_versions(paper_dict[\"Scholar articles\"])\n",
    "        paper_dict[\"Version urls\"] = version_list\n",
    "    else:\n",
    "        paper_dict[\"Version urls\"] = []\n",
    "\n",
    "pickle.dump(paper_dict_list, open(\"paper_urls.txt\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "website",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
