{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import typing\n",
    "import bs4\n",
    "import pickle\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "PROXY_URL = \"http://5zipXAuZVPsquwtL:wifi;;;;@proxy.froxy.com:9000\"\n",
    "SCHOLAR_PROFILE = \"qHFA5z4AAAAJ\"\n",
    "PROXIES = {\"https\": PROXY_URL, \"http\": PROXY_URL}\n",
    "HEADERS = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status = 200\n"
     ]
    }
   ],
   "source": [
    "with requests.post(\n",
    "    url=\"https://api.semanticscholar.org/graph/v1/author/batch\",\n",
    "    headers=HEADERS,\n",
    "    proxies=PROXIES,\n",
    "    json={\"ids\": [\"152307988\"]},\n",
    "    params={\"fields\": \"name,url,paperCount,hIndex,papers\"},\n",
    ") as response:\n",
    "\n",
    "    print(\"status =\", response.status_code)\n",
    "    author_dict = json.loads(response.text)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "status = 200\n"
     ]
    }
   ],
   "source": [
    "with requests.post(\n",
    "    url=\"https://api.semanticscholar.org/graph/v1/paper/batch\",\n",
    "    headers=HEADERS,\n",
    "    proxies=PROXIES,\n",
    "    json={\"ids\": [paper[\"paperId\"] for paper in author_dict[\"papers\"]]},\n",
    "    params={\"fields\": \"title,year,externalIds,publicationVenue,openAccessPdf\"},\n",
    ") as response:\n",
    "    print(\"status =\", response.status_code)\n",
    "    paper_dict = json.loads(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025 On Solving Minimization and Min-Max Problems by First-Order Methods with Relative Error in Gradients\n",
      "\t 2503.06628\n",
      "2025 Understanding Gradient Orthogonalization for Deep Learning via Non-Euclidean Trust-Region Optimization\n",
      "\t 2503.12645\n",
      "2024 Decentralized Optimization with Coupled Constraints\n",
      "\t 2407.02020\n",
      "2024 Decentralized Finite-Sum Optimization over Time-Varying Networks\n",
      "\t 2402.02490\n",
      "2024 Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "\t 2405.18031\n",
      "2023 Is Consensus Acceleration Possible in Decentralized Optimization over Slowly Time-Varying Networks?\n",
      "\t 2301.11817\n",
      "2023 Non-smooth setting of stochastic decentralized convex optimization problem over time-varying Graphs\n",
      "\t 2307.00392\n",
      "2022 Accelerated variance-reduced methods for saddle-point problems\n",
      "\t {'DBLP': 'journals/ejco/BorodichTTKGD22', 'DOI': '10.1016/j.ejco.2022.100048', 'CorpusId': 252891662}\n",
      "2022 Optimal Algorithms for Decentralized Stochastic Variational Inequalities\n",
      "\t 2202.02771\n",
      "2022 Smooth Monotone Stochastic Variational Inequalities and Saddle Point Problems - Survey\n",
      "\t 2208.13592\n",
      "2022 Optimal Gradient Sliding and its Application to Optimal Distributed Optimization Under Similarity\n",
      "\t {'DBLP': 'conf/nips/KovalevBBGS22', 'CorpusId': 258509705}\n",
      "2022 Decentralized convex optimization over time-varying graphs: a survey\n",
      "\t 2210.09719\n",
      "2022 An Optimal Algorithm for Strongly Convex Min-min Optimization\n",
      "\t 2212.14439\n",
      "2022 The First Optimal Algorithm for Smooth and Strongly-Convex-Strongly-Concave Minimax Optimization\n",
      "\t 2205.05653\n",
      "2022 Optimal Gradient Sliding and its Application to Distributed Optimization Under Similarity\n",
      "\t 2205.15136\n",
      "2022 Decentralized optimization over time-varying graphs\n",
      "\t {'CorpusId': 252968374}\n",
      "2022 Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "\t 2206.00090\n",
      "2022 Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "\t {'DBLP': 'journals/oms/HorvathKMRS23', 'DOI': '10.1080/10556788.2022.2117355', 'CorpusId': 252574277}\n",
      "2022 The First Optimal Acceleration of High-Order Methods in Smooth Convex Optimization\n",
      "\t 2205.09647\n",
      "2022 Communication Acceleration of Local Gradient Methods via an Accelerated Primal-Dual Algorithm with Inexact Prox\n",
      "\t 2207.03957\n",
      "2022 Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "\t 2205.15669\n",
      "2022 On Scaled Methods for Saddle Point Problems\n",
      "\t 2206.08303\n",
      "2022 Decentralized Computation of Wasserstein Barycenter over Time-Varying Networks\n",
      "\t {'CorpusId': 249209860}\n",
      "2021 On Accelerated Methods for Saddle-Point Problems with Composite Structure\n",
      "\t 2103.09344\n",
      "2021 IntSGD: Adaptive Floatless Compression of Stochastic Gradients\n",
      "\t 2102.08374\n",
      "2021 Accelerated Primal-Dual Gradient Method for Smooth and Convex-Concave Saddle-Point Problems with Bilinear Coupling\n",
      "\t 2112.15199\n",
      "2021 Lower Bounds and Optimal Algorithms for Smooth and Strongly Convex Decentralized Optimization Over Time-Varying Networks\n",
      "\t 2106.04469\n",
      "2021 Decentralized Distributed Optimization for Saddle Point Problems\n",
      "\t {'DBLP': 'journals/corr/abs-2102-07758', 'CorpusId': 269010418}\n",
      "2021 ADOM: Accelerated Decentralized Optimization Method for Time-Varying Networks\n",
      "\t 2102.09234\n",
      "2021 Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "\t 2102.07758\n",
      "2021 Near-Optimal Decentralized Algorithms for Saddle Point Problems over Time-Varying Networks\n",
      "\t 2107.05957\n",
      "2021 Optimal Decentralized Algorithms for Saddle Point Problems over Time-Varying Networks âˆ—\n",
      "\t {'CorpusId': 235828719}\n",
      "2021 An Optimal Algorithm for Strongly Convex Minimization under Affine Constraints\n",
      "\t 2102.11079\n",
      "2021 IntSGD: Floatless Compression of Stochastic Gradients\n",
      "\t {'DBLP': 'journals/corr/abs-2102-08374', 'CorpusId': 231933702}\n",
      "2020 Towards Accelerated Rates for Distributed Optimization over Time-Varying Networks\n",
      "\t 2009.11069\n",
      "2020 Fast Linear Convergence of Randomized BFGS\n",
      "\t 2002.11337\n",
      "2020 Acceleration for Compressed Gradient Descent in Distributed Optimization\n",
      "\t {'MAG': '3035688841', 'CorpusId': 225522642}\n",
      "2020 From Local SGD to Local Fixed Point Methods for Federated Learning\n",
      "\t 2004.01442\n",
      "2020 Accelerated Methods for Saddle-Point Problem\n",
      "\t {'MAG': '3112285046', 'DOI': '10.1134/S0965542520110020', 'CorpusId': 230525527}\n",
      "2020 Acceleration for Compressed Gradient Descent in Distributed and Federated Optimization\n",
      "\t 2002.11364\n",
      "2020 A Linearly Convergent Algorithm for Decentralized Optimization: Sending Less Bits for Free!\n",
      "\t 2011.01697\n",
      "2020 Variance Reduced Coordinate Descent with Acceleration: New Method With a Surprising Application to Finite-Sum Problems\n",
      "\t 2002.04670\n",
      "2020 Linearly Converging Error Compensated SGD\n",
      "\t 2010.12292\n",
      "2020 Optimal and Practical Algorithms for Smooth and Strongly Convex Decentralized Optimization\n",
      "\t 2006.11773\n",
      "2019 Stochastic Proximal Langevin Algorithm: Potential Splitting and Nonasymptotic Rates\n",
      "\t 1905.11768\n",
      "2019 Don't Jump Through Hoops and Remove Those Loops: SVRG and Katyusha are Better Without the Outer Loop\n",
      "\t 1901.08689\n",
      "2019 RSN: Randomized Subspace Newton\n",
      "\t 1905.10874\n",
      "2019 Accelerated methods for composite non-bilinear saddle point problem\n",
      "\t 1906.03620\n",
      "2019 Revisiting Stochastic Extragradient\n",
      "\t 1905.11373\n",
      "2019 Stochastic Distributed Learning with Gradient Quantization and Variance Reduction\n",
      "\t 1904.05115\n",
      "2019 Stochastic Newton and Cubic Newton Methods with Simple Local Linear-Quadratic Rates\n",
      "\t 1912.01597\n",
      "2019 Distributed Fixed Point Methods with Compressed Iterates\n",
      "\t 1912.09925\n",
      "2018 Stochastic Spectral and Conjugate Descent Methods\n",
      "\t 1802.03703\n",
      "2018 A hypothesis about the rate of global convergence for optimal methods (Newtons type) in smooth convex optimization\n",
      "\t {'MAG': '2859746396', 'DOI': '10.20537/2076-7633-2018-10-3-305-314', 'CorpusId': 126173357}\n"
     ]
    }
   ],
   "source": [
    "for paper in paper_dict:\n",
    "    print(paper[\"year\"], paper[\"title\"])\n",
    "    # if paper[\"publicationVenue\"]:\n",
    "    #     print(\"\\t\", paper[\"publicationVenue\"][\"name\"])\n",
    "    # else:\n",
    "    #     print(\"\\t\")\n",
    "    if \"ArXiv\" in paper[\"externalIds\"]:\n",
    "        print(\"\\t\", paper[\"externalIds\"][\"ArXiv\"])\n",
    "    else:\n",
    "        print(\"\\t\", paper[\"externalIds\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "website",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
