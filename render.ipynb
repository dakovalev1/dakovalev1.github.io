{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import bs4\n",
    "import pickle\n",
    "import urllib.parse\n",
    "import textwrap\n",
    "import dateparser\n",
    "import jinja2\n",
    "import datetime\n",
    "import unidecode\n",
    "\n",
    "\n",
    "PROXY_URL = \"http://5zipXAuZVPsquwtL:wifi;;;;@proxy.froxy.com:9000\"\n",
    "SCHOLAR_PROFILE = \"qHFA5z4AAAAJ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Title', 'Authors', 'Publication date', 'Journal', 'Volume', 'Issue', 'Pages', 'Publisher', 'Description', 'Total citations', 'Scholar articles', 'Version urls'])\n"
     ]
    }
   ],
   "source": [
    "paper_dict_list: typing.List[typing.Dict[str, bs4.Tag]] = pickle.load(\n",
    "    open(\"paper_urls.txt\", \"rb\")\n",
    ")\n",
    "print(paper_dict_list[0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUBLISHER_NETLOC = set(\n",
    "    [\n",
    "        \"proceedings.neurips.cc\",\n",
    "        \"proceedings.mlr.press\",\n",
    "        \"link.springer.com\",\n",
    "        \"ems.press\",\n",
    "        \"www.sciencedirect.com\",\n",
    "        \"www.tandfonline.com\",\n",
    "        \"crm-en.ics.org.ru\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class Paper:\n",
    "    def __init__(self, paper_dict: typing.Dict[str, bs4.Tag]) -> None:\n",
    "        self.title: str = paper_dict[\"Title\"].text\n",
    "        self.authors: str = paper_dict[\"Authors\"].text\n",
    "        self.year: int = dateparser.parse(paper_dict[\"Publication date\"].text).year\n",
    "\n",
    "        self.venue: str = \"\"\n",
    "        self.type: str = \"\"\n",
    "\n",
    "        if \"Journal\" in paper_dict.keys():\n",
    "            self.venue = paper_dict[\"Journal\"].text\n",
    "            if self.venue.lower().startswith(\"arxiv\"):\n",
    "                self.type = \"preprint\"\n",
    "            else:\n",
    "                self.type = \"publication\"\n",
    "        elif \"Conference\" in paper_dict.keys():\n",
    "            self.venue = paper_dict[\"Conference\"].text\n",
    "            self.type = \"publication\"\n",
    "        elif \"Book\" in paper_dict.keys():\n",
    "            self.venue = paper_dict[\"Book\"].text\n",
    "            self.type = \"publication\"\n",
    "        elif \"Institution\" in paper_dict.keys():\n",
    "            self.venue = paper_dict[\"Institution\"].text\n",
    "            self.type = \"thesis\"\n",
    "        else:\n",
    "            raise KeyError(\"unknown paper type\")\n",
    "\n",
    "        self.link: str | None = None\n",
    "        self.openreview: str | None = None\n",
    "        self.arxiv: str | None = None\n",
    "        self.pdf: str | None = None\n",
    "\n",
    "        for url in paper_dict[\"Version urls\"]:\n",
    "            url_parsed: urllib.parse.ParseResult = urllib.parse.urlparse(url)\n",
    "            if self.arxiv is None and url_parsed.netloc == \"arxiv.org\":\n",
    "                self.arxiv = url\n",
    "            if self.openreview is None and url_parsed.netloc == \"openreview.net\":\n",
    "                self.openreview = url\n",
    "            if self.link is None and url_parsed.netloc in PUBLISHER_NETLOC:\n",
    "                self.link = url\n",
    "            if self.pdf is None and url_parsed.path.lower().endswith(\"pdf\"):\n",
    "                self.pdf = url\n",
    "\n",
    "        if not self.arxiv is None:\n",
    "            self.pdf = self.arxiv.replace(\"abs\", \"pdf\")\n",
    "\n",
    "        print(self.title)\n",
    "        for url in paper_dict[\"Version urls\"]:\n",
    "            print(\"\\t\", url)\n",
    "\n",
    "    def to_html(self) -> str:\n",
    "        span_title = bs4.Tag(name=\"span\", attrs={\"class\": \"span-paper-title\"})\n",
    "\n",
    "        if self.link is None:\n",
    "            span_title.append(self.title)\n",
    "        else:\n",
    "            a_title = bs4.Tag(name=\"a\", attrs={\"href\": self.link})\n",
    "            a_title.append(self.title)\n",
    "            span_title.append(a_title)\n",
    "\n",
    "        span = bs4.Tag(name=\"span\")\n",
    "        span.append(f\" ({self.authors}). {self.venue}, {self.year}.\")\n",
    "\n",
    "        links: typing.List[str] = []\n",
    "\n",
    "        if self.type != \"preprint\" and not self.link is None:\n",
    "            a = bs4.Tag(name=\"a\", attrs={\"href\": self.link})\n",
    "            icon = bs4.Tag(name=\"i\", attrs={\"class\": \"fa fa-solid fa-link\"})\n",
    "            a.append(\"Link\")\n",
    "            a.append(icon)\n",
    "            links.append(a)\n",
    "\n",
    "        if not self.arxiv is None:\n",
    "            a = bs4.Tag(name=\"a\", attrs={\"href\": self.arxiv})\n",
    "            icon = bs4.Tag(name=\"i\", attrs={\"class\": \"ai ai-arxiv\"})\n",
    "            a.append(\"arXiv\")\n",
    "            a.append(icon)\n",
    "            links.append(a)\n",
    "\n",
    "        if not self.pdf is None:\n",
    "            a = bs4.Tag(name=\"a\", attrs={\"href\": self.pdf})\n",
    "            icon = bs4.Tag(\n",
    "                name=\"i\",\n",
    "                attrs={\n",
    "                    \"class\": \"fa fa-regular fa-file-pdf\",\n",
    "                    # \"style\": \"margin-left:0.1rem;\",\n",
    "                },\n",
    "            )\n",
    "            a.append(\"PDF\")\n",
    "            a.append(icon)\n",
    "            links.append(a)\n",
    "\n",
    "        if len(links):\n",
    "            span.append(bs4.BeautifulSoup(\"<br>\", \"html.parser\"))\n",
    "            # span.append(\" \")\n",
    "            for i in range(len(links)):\n",
    "                links[i][\"class\"] = \"paper-link\"\n",
    "                div = bs4.Tag(\n",
    "                    name=\"div\",\n",
    "                    attrs={\"style\": \"display:inline-block\"},\n",
    "                )\n",
    "                # if i == 0:\n",
    "                #     div.append(\"[\")\n",
    "                div.append(links[i])\n",
    "                # if i + 1 != len(links):\n",
    "                #     div.append(\",\")\n",
    "                # else:\n",
    "                #     div.append(\"]\")\n",
    "                span.append(div)\n",
    "                if i + 1 != len(links):\n",
    "                    span.append(\" \")\n",
    "\n",
    "        soup = bs4.BeautifulSoup()\n",
    "        soup.append(span_title)\n",
    "        soup.append(span)\n",
    "\n",
    "        print(soup, links)\n",
    "        return soup\n",
    "\n",
    "    def to_latex(self) -> str:\n",
    "        latex: str = \"\"\n",
    "\n",
    "        latex_link = None\n",
    "\n",
    "        if self.link != None:\n",
    "            latex_link = self.link\n",
    "        elif self.arxiv != None:\n",
    "            latex_link = self.arxiv\n",
    "\n",
    "        if latex_link is None:\n",
    "            latex += f\"\\\\textbf{{{self.title}}}\"\n",
    "        else:\n",
    "            latex += f\"\\\\textbf{{\\\\href{{{latex_link}}}{{{self.title}}}}}\"\n",
    "\n",
    "        latex += f\" ({unidecode.unidecode(self.authors)}). \\\\textit{{{self.venue}}}, {self.year}.\"\n",
    "        return latex\n",
    "\n",
    "    def __lt__(self, other: typing.Self) -> bool:\n",
    "        return (\n",
    "            self.year > other.year\n",
    "            or (self.year == other.year and self.venue < other.venue)\n",
    "            or (\n",
    "                self.year == other.year\n",
    "                and self.venue == other.venue\n",
    "                and self.title < other.title\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return '<Paper title=\"{}\" authors=\"{}\" venue=\"{}\" year = {}>'.format(\n",
    "            textwrap.shorten(self.title, width=30, placeholder=\"...\"),\n",
    "            textwrap.shorten(self.authors, width=30, placeholder=\"...\"),\n",
    "            self.venue,\n",
    "            self.year,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stochastic distributed learning with gradient quantization and double-variance reduction\n",
      "\t https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355\n",
      "\t https://publications.cispa.saarland/id/eprint/3980\n",
      "\t https://www.ingentaconnect.com/content/tandf/goms/2023/00000038/00000001/art00004\n",
      "\t https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=10556788&AN=162079596&h=YfHPyPaPVyt%2BvD6b6mDQPcaX%2Fdv2YuAvJcpGy5uGvEtN6voQd6ZWIfJ2Z3O63CJVVLvNXEVbFhPBNWx39f5gBw%3D%3D&crl=c\n",
      "\t https://faculty.kaust.edu.sa/en/publications/stochastic-distributed-learning-with-gradient-quantization-and-do\n",
      "\t https://core.ac.uk/download/pdf/599569336.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstreams/ec30a89d-283a-4209-a6e6-8f5ac73c2f7f/download\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190405115H/abstract\n",
      "\t https://publications.cispa.de/articles/journal_contribution/Stochastic_distributed_learning_with_gradient_quantization_and_double-variance_reduction/25469572\n",
      "\t https://nchr.elsevierpure.com/en/publications/stochastic-distributed-learning-with-gradient-quantization-and-do\n",
      "\t https://www.researchgate.net/profile/Samuel-Horvath-2/publication/332342206_Stochastic_Distributed_Learning_with_Gradient_Quantization_and_Variance_Reduction/links/5d359e724585153e5916c157/Stochastic-Distributed-Learning-with-Gradient-Quantization-and-Variance-Reduction.pdf\n",
      "\t https://publications.cispa.saarland/3980/\n",
      "\t https://repository.kaust.edu.sa/handle/10754/653103\n",
      "\t https://publications.cispa.de/ndownloader/files/45257332\n",
      "Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop\n",
      "\t https://proceedings.mlr.press/v117/kovalev20a.html\n",
      "\t https://arxiv.org/abs/1901.08689\n",
      "\t http://dml.mathdoc.fr/item/1901.08689/\n",
      "\t http://proceedings.mlr.press/v117/kovalev20a.html\n",
      "\t https://faculty.kaust.edu.sa/en/publications/dont-jump-through-hoops-and-remove-those-loops-svrg-and-katyusha-\n",
      "\t https://repository.kaust.edu.sa/handle/10754/653122\n",
      "\t https://www.researchgate.net/profile/Samuel-Horvath-2/publication/330673142_Don't_Jump_Through_Hoops_and_Remove_Those_Loops_SVRG_and_Katyusha_are_Better_Without_the_Outer_Loop/links/5d359eaf92851cd0467b96a5/Dont-Jump-Through-Hoops-and-Remove-Those-Loops-SVRG-and-Katyusha-are-Better-Without-the-Outer-Loop.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190108689K/abstract\n",
      "\t https://repository.kaust.edu.sa/bitstreams/cad85c90-bfce-430d-ad1a-f74af61a3f4c/download\n",
      "Acceleration for compressed gradient descent in distributed and federated optimization\n",
      "\t https://arxiv.org/abs/2002.11364\n",
      "\t https://elibrary.ru/item.asp?id=45863264\n",
      "\t https://pdfs.semanticscholar.org/bd68/6257ad1f948a30a37c32f91abb388ce62e1e.pdf\n",
      "\t https://ink.library.smu.edu.sg/sis_research/8681/\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3524938.3525485\n",
      "\t https://proceedings.mlr.press/v119/li20g.html\n",
      "\t https://faculty.kaust.edu.sa/en/publications/acceleration-for-compressed-gradient-descent-in-distributed-and-f\n",
      "\t https://www.researchgate.net/profile/Zhize-Li/publication/339526946_Acceleration_for_Compressed_Gradient_Descent_in_Distributed_and_Federated_Optimization/links/5e59fe2b4585152ce8f85f96/Acceleration-for-Compressed-Gradient-Descent-in-Distributed-and-Federated-Optimization.pdf\n",
      "\t https://zhizeli.github.io/files/acgd_slides.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200211364L/abstract\n",
      "\t https://smusg.elsevierpure.com/en/publications/acceleration-for-compressed-gradient-descent-in-distributed-and-f\n",
      "\t https://icml.cc/media/Slides/icml/2020/virtual(no-parent)-15-19-00UTC-6191-acceleration_fo.pdf\n",
      "\t https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?params=/context/sis_research/article/9684/&path_info=ICML20_full_adiana.pdf\n",
      "\t https://icml.cc/media/icml-2020/Slides/6191.pdf\n",
      "\t http://proceedings.mlr.press/v119/li20g.html\n",
      "\t https://repository.kaust.edu.sa/handle/10754/662100\n",
      "From local SGD to local fixed-point methods for federated learning\n",
      "\t http://proceedings.mlr.press/v119/malinovskiy20a.html\n",
      "\t https://elibrary.ru/item.asp?id=46742951\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200401442M/abstract\n",
      "\t https://faculty.kaust.edu.sa/en/publications/from-local-sgd-to-local-fixed-point-methods-for-federated-learnin\n",
      "\t https://www.researchgate.net/profile/Grigory-Malinovskiy/publication/340452734_From_Local_SGD_to_Local_Fixed_Point_Methods_for_Federated_Learning/links/5f0974b145851550509c7be9/From-Local-SGD-to-Local-Fixed-Point-Methods-for-Federated-Learning.pdf\n",
      "\t http://proceedings.mlr.press/v119/malinovskiy20a/malinovskiy20a-supp.pdf\n",
      "\t https://repository.kaust.edu.sa/handle/10754/662492\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3524938.3525559\n",
      "\t https://arxiv.org/abs/2004.01442\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/a75121f4-6d92-4f67-8044-762506afe24b/content\n",
      "RSN: randomized subspace Newton\n",
      "\t https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\n",
      "\t https://faculty.kaust.edu.sa/en/publications/rsn-randomized-subspace-newton\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3454287.3454343\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\n",
      "\t https://richtarik.org/posters/Poster-RSN.pdf\n",
      "\t https://hal.science/hal-02365297/\n",
      "\t https://arxiv.org/abs/1905.10874\n",
      "\t https://openreview.net/forum?id=ByfmEEreIS\n",
      "\t https://gowerrobert.github.io/pdf/posters/RSN_neurips_poster.pdf\n",
      "\t http://papers.neurips.cc/paper/8351-rsn-randomized-subspace-newton.pdf\n",
      "\t https://telecom-paris.hal.science/hal-02365297/\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190510874G/abstract\n",
      "\t https://elibrary.ru/item.asp?id=45384452\n",
      "\t https://repository.kaust.edu.sa/handle/10754/660275\n",
      "\t https://repository.kaust.edu.sa/handle/10754/670890\n",
      "Revisiting stochastic extragradient\n",
      "\t http://proceedings.mlr.press/v108/mishchenko20a\n",
      "\t https://repository.kaust.edu.sa/handle/10754/660278\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190511373M/abstract\n",
      "\t https://www.academia.edu/download/82489114/1905.11373.pdf\n",
      "\t http://proceedings.mlr.press/v108/mishchenko20a.html\n",
      "\t https://faculty.kaust.edu.sa/en/publications/revisiting-stochastic-extragradient\n",
      "\t https://www.konstmish.com/uploads/slides/20-extra.pdf\n",
      "\t https://arxiv.org/abs/1905.11373\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/0e88e647-6b7b-42d9-873a-19bf74d49a48/content\n",
      "Linearly converging error compensated SGD\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\n",
      "\t https://openreview.net/forum?id=t6y7IjScg2m\n",
      "\t https://eduardgorbunov.github.io/assets/files/neurips_after_party_2021.pdf\n",
      "\t https://repository.kaust.edu.sa/handle/10754/666025\n",
      "\t https://proceedings.neurips.cc/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\n",
      "\t https://pdfs.semanticscholar.org/a6d2/462e8b1777a896058b184923f9129c63a794.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3495724.3497478\n",
      "\t https://faculty.kaust.edu.sa/en/publications/linearly-converging-error-compensated-sgd\n",
      "\t https://elibrary.ru/item.asp?id=46790933\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv201012292G/abstract\n",
      "\t https://arxiv.org/abs/2010.12292\n",
      "\t https://repository.kaust.edu.sa/items/07fb6f12-7e0e-4e0e-8167-cb596ec3d97b\n",
      "\t https://eduardgorbunov.github.io/assets/files/ef_sigma_k_poster.pdf\n",
      "Optimal and practical algorithms for smooth and strongly convex decentralized optimization\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\n",
      "\t https://arxiv.org/abs/2006.11773\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3495724.3497264\n",
      "\t https://papers.neurips.cc/paper_files/paper/2020/file/d530d454337fb09964237fecb4bea6ce-Paper.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200611773K/abstract\n",
      "\t https://faculty.kaust.edu.sa/en/publications/optimal-and-practical-algorithms-for-smooth-and-strongly-convex-d\n",
      "\t https://proceedings.neurips.cc/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\n",
      "\t https://repository.kaust.edu.sa/handle/10754/666022\n",
      "A linearly convergent algorithm for decentralized optimization: Sending less bits for free!\n",
      "\t https://proceedings.mlr.press/v130/kovalev21a.html\n",
      "\t https://repository.kaust.edu.sa/handle/10754/665879\n",
      "\t https://richtarik.org/posters/Poster-Decentralized-DIANA.pdf\n",
      "\t https://arxiv.org/abs/2011.01697\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv201101697K/abstract\n",
      "\t https://faculty.kaust.edu.sa/en/publications/a-linearly-convergent-algorithm-for-decentralized-optimization-se\n",
      "\t http://proceedings.mlr.press/v130/kovalev21a\n",
      "\t https://repository.kaust.edu.sa/bitstreams/e5c7d805-a861-4dc8-a17f-cf7d6e58d3d2/download\n",
      "Accelerated methods for saddle-point problem\n",
      "\t https://link.springer.com/article/10.1134/S0965542520110020\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020CMMPh..60.1787A/abstract\n",
      "\t https://elibrary.ru/item.asp?id=45099936\n",
      "\t https://www.mathnet.ru/eng/zvmmf11157\n",
      "\t https://repository.kaust.edu.sa/items/530408e2-12bf-4f56-9883-8a77e5806224\n",
      "\t https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=09655425&AN=147479258&h=DqWtxV31KzxKDWR3KUGun43VDY8bDJ%2Bz9%2FS95X7SVsRR8f7JJybjHTMlbrRaetihd%2BenJ8EVa0B6Vx0EnF63Ow%3D%3D&crl=c\n",
      "\t https://repository.kaust.edu.sa/handle/10754/666346\n",
      "\t https://www.mathnet.ru/eng/zvmmf/v60/i11/p1843\n",
      "\t https://arxiv.org/abs/1906.03620\n",
      "\t https://archive.wias-berlin.de/receive/wias_mods_00007103\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190603620A/abstract\n",
      "Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates\n",
      "\t https://arxiv.org/abs/1912.01597\n",
      "\t https://repository.kaust.edu.sa/handle/10754/660727\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv191201597K/abstract\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/766f5e70-bb84-4c1b-81ad-efd9c3aaa6b3/content\n",
      "Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html\n",
      "\t https://arxiv.org/abs/2112.15199\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/file/883f66687a521536c505f9b2fbdcbf1e-Supplemental-Conference.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3601849\n",
      "\t https://repository.kaust.edu.sa/handle/10754/692832\n",
      "\t https://faculty.kaust.edu.sa/en/publications/accelerated-primal-dual-gradient-method-for-smooth-and-convex-con\n",
      "\t https://labmmo.ru/upload/000/u8/6/f/2112-15199.pdf\n",
      "\t https://openreview.net/forum?id=FncDhRcRYiN\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv211215199K/abstract\n",
      "\t https://repository.kaust.edu.sa/items/6c8bb12b-9753-440e-a112-9f3d9a4244a6\n",
      "Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks\n",
      "\t https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html\n",
      "\t https://faculty.kaust.edu.sa/en/publications/lower-bounds-and-optimal-algorithms-for-smooth-and-strongly-conve\n",
      "\t https://repository.kaust.edu.sa/bitstreams/239fe39a-dabf-4148-a5bf-fdc56ef785b1/download\n",
      "\t https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html?roistat_visit=7938444\n",
      "\t https://elibrary.ru/item.asp?id=49123379\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210604469K/abstract\n",
      "\t https://arxiv.org/abs/2106.04469\n",
      "\t https://labmmo.ru/upload/000/u8/3/8/2106-04469.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3540261.3541971\n",
      "\t https://papers.neurips.cc/paper/2021/file/bc37e109d92bdc1ea71da6c919d54907-Paper.pdf\n",
      "\t https://proceedings.nips.cc/paper_files/paper/2021/file/bc37e109d92bdc1ea71da6c919d54907-Supplemental.pdf\n",
      "\t https://fl-icml.github.io/2021/papers/FL-ICML21_paper_46.pdf\n",
      "\t https://openreview.net/forum?id=L8-54wkift\n",
      "\t https://papers.neurips.cc/paper_files/paper/2021/file/bc37e109d92bdc1ea71da6c919d54907-Paper.pdf\n",
      "\t https://openreview.net/forum?id=V0QJvzu6Uh\n",
      "\t https://proceedings.nips.cc/paper/2021/file/bc37e109d92bdc1ea71da6c919d54907-Paper.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/239fe39a-dabf-4148-a5bf-fdc56ef785b1/content\n",
      "Decentralized distributed optimization for saddle point problems\n",
      "\t https://arxiv.org/abs/2102.07758\n",
      "\t https://www.researchgate.net/profile/Pavel-Dvurechensky/publication/349335061_Decentralized_Distributed_Optimization_for_Saddle_Point_Problems/links/6076cc65a03fca55fe29919e/Decentralized-Distributed-Optimization-for-Saddle-Point-Problems.pdf\n",
      "\t https://repository.kaust.edu.sa/handle/10754/667615\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210207758R/abstract\n",
      "\t https://archive.wias-berlin.de/servlets/MCRFileNodeServlet/wias_derivate_00003520/2102.07758.pdf\n",
      "\t https://archive.wias-berlin.de/receive/wias_mods_00007121\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/ff50fd83-96e5-4515-bd72-0ea1472bb07f/content\n",
      "The first optimal acceleration of high-order methods in smooth convex optimization\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220509647K/abstract\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3602831\n",
      "\t https://openreview.net/forum?id=YgmiL2Ur01P\n",
      "\t https://arxiv.org/abs/2205.09647\n",
      "\t https://repository.kaust.edu.sa/handle/10754/678185\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/e56f394bbd4f0ec81393d767caa5a31b-Supplemental-Conference.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/b4044a21-effb-419e-83f8-9c6fa796386b/content\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/e56f394bbd4f0ec81393d767caa5a31b-Paper-Conference.pdf\n",
      "Smooth monotone stochastic variational inequalities and saddle point problems: A survey\n",
      "\t https://ems.press/journals/mag/articles/9939904\n",
      "\t https://faculty.kaust.edu.sa/en/publications/smooth-monotone-stochastic-variational-inequalities-and-saddle-po\n",
      "\t https://arxiv.org/abs/2208.13592\n",
      "\t https://euromathsoc.org/magazine/articles/112\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220813592B/abstract\n",
      "\t https://content.ems.press/assets/public/full-texts/serials/mag/127/9939904/online/10.4171-mag-112.pdf\n",
      "\t https://search.ebscohost.com/login.aspx?direct=true&profile=ehost&scope=site&authtype=crawler&jrnl=27477894&AN=162616555&h=tnu7DQT%2FtaMAc%2BcJ39Aiq1mesXb5aeiI6rsX0jtvF0HCnRDm6usm3QD3X7MycRMVzenb7P4%2BNBuOq9UgPdD7vg%3D%3D&crl=c\n",
      "\t https://ems.press/content/serial-article-files/27058?nt=1\n",
      "\t https://repository.kaust.edu.sa/handle/10754/680934\n",
      "\t https://content.ems.press/assets/public/full-texts/serials/mag/127/9939904/online-first/10.4171-mag-112-online-first.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstreams/1807f103-eedb-4422-9737-399aa9efe294/download\n",
      "Optimal gradient sliding and its application to optimal distributed optimization under similarity\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220515136K/abstract\n",
      "\t https://labmmo.ru/upload/000/u8/5/3/1749-optimal-gradient-sliding-and-i.pdf\n",
      "\t https://openreview.net/forum?id=QrK0WDLVHZt\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3602697\n",
      "\t https://arxiv.org/abs/2205.15136\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/d88f6f81e1aaf606776ffdd06fdf24ef-Paper-Conference.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstreams/d7ccef9c-c013-4449-9333-537404d609bb/download\n",
      "\t https://faculty.kaust.edu.sa/en/publications/optimal-gradient-sliding-and-its-application-to-distributed-optim\n",
      "\t https://repository.kaust.edu.sa/handle/10754/678602\n",
      "ADOM: accelerated decentralized optimization method for time-varying networks\n",
      "\t http://proceedings.mlr.press/v139/kovalev21a\n",
      "\t https://shulgin-egor.github.io/files/adom_poster.pdf\n",
      "\t https://icml.cc/media/icml-2021/Slides/10125.pdf\n",
      "\t https://labmmo.ru/upload/000/u8/c/2/2102-09234.pdf\n",
      "\t https://faculty.kaust.edu.sa/en/publications/adom-accelerated-decentralized-optimization-method-for-time-varyi\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210209234K/abstract\n",
      "\t https://repository.kaust.edu.sa/bitstreams/d0a7e842-7669-49ac-9ada-ae82ce9280f5/download\n",
      "\t http://proceedings.mlr.press/v139/kovalev21a/kovalev21a-supp.pdf\n",
      "\t https://richtarik.org/posters/Poster-ADOM-ICML-2021.pdf\n",
      "\t https://arxiv.org/abs/2102.09234\n",
      "\t https://shulgin-egor.github.io/publication/adom/adom_poster.pdf\n",
      "\t https://repository.kaust.edu.sa/handle/10754/667661\n",
      "Optimal algorithms for decentralized stochastic variational inequalities\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html\n",
      "\t https://faculty.kaust.edu.sa/en/publications/optimal-algorithms-for-decentralized-stochastic-variational-inequ\n",
      "\t https://repository.kaust.edu.sa/handle/10754/677972\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220202771K/abstract\n",
      "\t https://arxiv.org/abs/2202.02771\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3602523\n",
      "\t https://openreview.net/forum?id=omI5hgwgrsa\n",
      "\t https://labmmo.ru/upload/000/u8/2/2/2202-02771.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/b402bccc-272c-4cfb-acb5-1cb3a2d745f0/content\n",
      "Accelerated variance-reduced methods for saddle-point problems\n",
      "\t https://arxiv.org/abs/2103.09344\n",
      "\t http://vst.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\n",
      "\t https://archive.wias-berlin.de/servlets/MCRFileNodeServlet/wias_derivate_00003533/2103.09344.pdf\n",
      "\t https://labmmo.ru/upload/000/u8/b/a/2103-09344.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210309344T/abstract\n",
      "\t https://repository.kaust.edu.sa/bitstreams/b68483c9-c882-4ef4-a18d-de9e30fa7f8c/download\n",
      "\t https://www.academia.edu/download/96798398/2103.09344v2.pdf\n",
      "\t https://www.mathnet.ru/eng/crm1069\n",
      "\t http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\n",
      "\t https://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\n",
      "\t https://labmmo.ru/upload/000/u8/1/9/22-tominin.pdf\n",
      "\t https://crm.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\n",
      "\t https://archive.wias-berlin.de/receive/wias_mods_00007134\n",
      "\t https://www.sciencedirect.com/science/article/pii/S2192440622000247\n",
      "\t https://faculty.kaust.edu.sa/en/publications/accelerated-variance-reduced-methods-for-saddle-point-problems\n",
      "\t https://archive.wias-berlin.de/receive/wias_mods_00007698\n",
      "\t https://repository.kaust.edu.sa/handle/10754/685261\n",
      "\t https://labmmo.ru/upload/000/u8/6/3/1-s2-0-s2192440622000247-main.pdf\n",
      "\t https://oa.tib.eu/renate/handle/123456789/11625\n",
      "\t https://oa.tib.eu/renate/items/77ca6603-57d9-478e-9741-a1c031174e05\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/94f2de67-b691-4460-a94d-208468c92094/content\n",
      "\t https://faculty.kaust.edu.sa/en/publications/accelerated-variance-reduced-methods-for-saddle-point-problems-2\n",
      "\t https://elibrary.ru/item.asp?id=53767554\n",
      "\t https://crm.ics.org.ru/journal/article/3325/\n",
      "\t https://repository.kaust.edu.sa/handle/10754/693497\n",
      "\t https://repository.kaust.edu.sa/items/37b99088-d0fb-4ab9-a25e-31eb3e84198d\n",
      "IntSGD: Adaptive floatless compression of stochastic gradients\n",
      "\t https://arxiv.org/abs/2102.08374\n",
      "\t https://openreview.net/forum?id=pFyXqxChZc\n",
      "\t https://repository.kaust.edu.sa/handle/10754/667733\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210208374M/abstract\n",
      "\t https://www.konstmish.com/uploads/slides/22-intsgd-slides.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstreams/5849784f-31de-480c-a3ed-a80e47f2d376/download\n",
      "\t https://repository.kaust.edu.sa/bitstreams/c2660e1d-6c39-4815-8bfe-71587f858327/download\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/c2660e1d-6c39-4815-8bfe-71587f858327/content\n",
      "An optimal algorithm for strongly convex minimization under affine constraints\n",
      "\t https://proceedings.mlr.press/v151/salim22a.html\n",
      "\t https://arxiv.org/abs/2102.11079\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210211079S/abstract\n",
      "\t https://faculty.kaust.edu.sa/en/publications/an-optimal-algorithm-for-strongly-convex-minimization-under-affin\n",
      "\t https://lcondat.github.io/publis/Salim_AISTATS_2022.pdf\n",
      "\t https://repository.kaust.edu.sa/bitstreams/d4e147fb-6317-4dfc-ba50-33777751bda0/download\n",
      "Towards accelerated rates for distributed optimization over time-varying networks\n",
      "\t https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200911069R/abstract\n",
      "\t https://elibrary.ru/item.asp?id=47535257\n",
      "\t https://arxiv.org/abs/2009.11069\n",
      "\t https://repository.kaust.edu.sa/handle/10754/665439\n",
      "\t https://books.google.com/books?hl=en&lr=&id=6HlMEAAAQBAJ&oi=fnd&pg=PA258&ots=aRrc2EwJTk&sig=652vB10PDx8FVsAHtwiJvUxv8DM\n",
      "\t https://link.springer.com/content/pdf/10.1007/978-3-030-91059-4.pdf#page=272\n",
      "\t https://repository.kaust.edu.sa/bitstreams/b3e2f77f-46f4-4d70-a803-a0d831c78d26/download\n",
      "Stochastic proximal Langevin algorithm: Potential splitting and nonasymptotic rates\n",
      "\t https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\n",
      "\t http://papers.neurips.cc/paper/8891-stochastic-proximal-langevin-algorithm-potential-splitting-and-nonasymptotic-rates.pdf\n",
      "\t https://elibrary.ru/item.asp?id=45347186\n",
      "\t https://repository.kaust.edu.sa/handle/10754/660280\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv190511768S/abstract\n",
      "\t https://adil-salim.github.io/Research/langevin19.pdf\n",
      "\t https://arxiv.org/abs/1905.11768\n",
      "\t https://faculty.kaust.edu.sa/en/publications/stochastic-proximal-langevin-algorithm-potential-splitting-and-no\n",
      "\t https://openreview.net/forum?id=HklDsNrlIr\n",
      "\t https://richtarik.org/posters/Poster-SPLA.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3454287.3454884\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/370aa404-7758-4366-b92f-f73bc2147526/content\n",
      "The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220505653K/abstract\n",
      "\t https://repository.kaust.edu.sa/handle/10754/677973\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3600270.3601338\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/file/5e2ed801f62102f531d109d7c6e1b62f-Supplemental-Conference.pdf\n",
      "\t https://openreview.net/forum?id=pD5Pl5hen_g\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/242575fb-ca9c-4dee-b1f5-1eff8963d6b5/content\n",
      "Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html\n",
      "\t https://faculty.kaust.edu.sa/en/publications/communication-acceleration-of-local-gradient-methods-via-an-accel\n",
      "\t https://repository.kaust.edu.sa/handle/10754/679905\n",
      "\t https://openreview.net/forum?id=W72rB0wwLVu\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/88c3c482430a62d35e03926a22e4b67e-Supplemental-Conference.pdf\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2022/file/88c3c482430a62d35e03926a22e4b67e-Supplemental-Conference.pdf\n",
      "\t https://papers.neurips.cc/paper_files/paper/2022/file/88c3c482430a62d35e03926a22e4b67e-Paper-Conference.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/d6ec3d8a-67f3-4a37-a713-a3cf0c317e1c/content\n",
      "Distributed fixed point methods with compressed iterates\n",
      "\t https://arxiv.org/abs/1912.09925\n",
      "\t https://ui.adsabs.harvard.edu/abs/2019arXiv191209925C/abstract\n",
      "\t https://richtarik.org/papers/DFPMCI-new.pdf\n",
      "\t https://richtarik.org/papers/FPMCI.pdf\n",
      "Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems\n",
      "\t https://proceedings.mlr.press/v119/hanzely20b.html\n",
      "\t https://arxiv.org/abs/2002.04670\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3524938.3525316\n",
      "\t https://ui.adsabs.harvard.edu/abs/2020arXiv200204670H/abstract\n",
      "\t http://proceedings.mlr.press/v119/hanzely20b/hanzely20b-supp.pdf\n",
      "\t https://faculty.kaust.edu.sa/en/publications/variance-reduced-coordinate-descent-with-acceleration-new-method-\n",
      "\t https://repository.kaust.edu.sa/handle/10754/666019\n",
      "\t https://elibrary.ru/item.asp?id=45957891\n",
      "\t https://repository.kaust.edu.sa/bitstreams/7039c4f0-fde4-4d15-af90-d77de5ad3be3/download\n",
      "Fast linear convergence of randomized BFGS\n",
      "\t https://arxiv.org/abs/2002.11337\n",
      "\t https://repository.kaust.edu.sa/handle/10754/662101\n",
      "\t https://www.researchgate.net/profile/Robert-Gower-2/publication/339526612_Fast_Linear_Convergence_of_Randomized_BFGS/links/5edfa778299bf1d20bdb9443/Fast-Linear-Convergence-of-Randomized-BFGS.pdf\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/67773374-789f-4805-9fca-8d719335730e/content\n",
      "Near-optimal decentralized algorithms for saddle point problems over time-varying networks\n",
      "\t https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18\n",
      "\t https://repository.kaust.edu.sa/handle/10754/670330\n",
      "\t https://dl.acm.org/doi/abs/10.1007/978-3-030-91059-4_18\n",
      "\t https://labmmo.ru/upload/000/u8/5/7/2107-05957.pdf\n",
      "\t https://faculty.kaust.edu.sa/en/publications/near-optimal-decentralized-algorithms-for-saddle-point-problems-o\n",
      "\t https://ui.adsabs.harvard.edu/abs/2021arXiv210705957B/abstract\n",
      "\t https://arxiv.org/abs/2107.05957\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/df404c69-d9df-419d-9f5c-7e553aa2a7b4/content\n",
      "\t https://books.google.com/books?hl=en&lr=&id=6HlMEAAAQBAJ&oi=fnd&pg=PA246&ots=aRrc2EwJUi&sig=5XXBcV_YL8CHyojJ8jVMlYq4SpA\n",
      "\t https://repository.kaust.edu.sa/bitstreams/f87dc34c-5e44-40e4-9209-4c8f8e6887ff/download\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/f87dc34c-5e44-40e4-9209-4c8f8e6887ff/content\n",
      "Decentralized saddle-point problems with different constants of strong convexity and strong concavity\n",
      "\t https://link.springer.com/article/10.1007/s10287-023-00485-9\n",
      "\t https://arxiv.org/abs/2206.00090\n",
      "\t https://econpapers.repec.org/article/sprcomgts/v_3a21_3ay_3a2024_3ai_3a1_3ad_3a10.1007_5fs10287-023-00485-9.htm\n",
      "\t https://ideas.repec.org/a/spr/comgts/v21y2024i1d10.1007_s10287-023-00485-9.html\n",
      "\t https://repository.kaust.edu.sa/handle/10754/695659\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220600090M/abstract\n",
      "\t https://search.proquest.com/openview/59dbe27f2e36f1148bf160529816df7f/1?pq-origsite=gscholar&cbl=54341\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/9c2e58bc-790f-485b-b53b-1bd524f1b725/content\n",
      "Decentralized convex optimization over time-varying graphs\n",
      "\t https://link.springer.com/rwe/10.1007/978-3-030-54621-2_860-1\n",
      "\t https://arxiv.org/abs/2210.09719\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv221009719R/abstract\n",
      "Stochastic spectral and conjugate descent methods\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3327144.3327255\n",
      "\t https://ui.adsabs.harvard.edu/abs/2018arXiv180203703K/abstract\n",
      "\t https://repository.kaust.edu.sa/handle/10754/627183\n",
      "\t https://www.researchgate.net/profile/Eduard-Gorbunov/publication/323141815_Stochastic_Spectral_and_Conjugate_Descent_Methods/links/5ae987fb45851588dd820f3c/Stochastic-Spectral-and-Conjugate-Descent-Methods.pdf\n",
      "\t https://arxiv.org/abs/1802.03703\n",
      "\t https://richtarik.org/posters/Poster-SSCD.pdf\n",
      "\t http://papers.neurips.cc/paper/7596-stochastic-spectral-and-conjugate-descent-methods.pdf\n",
      "\t https://elibrary.ru/item.asp?id=38676976\n",
      "Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs\n",
      "\t https://link.springer.com/article/10.1007/s10287-023-00479-7\n",
      "\t https://ideas.repec.org/a/spr/comgts/v20y2023i1d10.1007_s10287-023-00479-7.html\n",
      "\t https://search.proquest.com/openview/351eef310a69e6187861d19131cbaab5/1?pq-origsite=gscholar&cbl=54341\n",
      "\t https://ui.adsabs.harvard.edu/abs/2023arXiv230700392L/abstract\n",
      "\t https://labmmo.ru/upload/000/u8/c/8/2307-00392.pdf\n",
      "\t https://econpapers.repec.org/article/sprcomgts/v_3a20_3ay_3a2023_3ai_3a1_3ad_3a10.1007_5fs10287-023-00479-7.htm\n",
      "\t https://arxiv.org/abs/2307.00392\n",
      "On scaled methods for saddle point problems\n",
      "\t https://arxiv.org/abs/2206.08303\n",
      "\t https://repository.kaust.edu.sa/handle/10754/679188\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220608303B/abstract\n",
      "\t https://openreview.net/forum?id=DEVBzx08Yl\n",
      "\t https://dclibrary.mbzuai.ac.ae/mlfp/156/\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/af3ba327-cd48-4690-8961-d7131807f31b/content\n",
      "Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization\n",
      "\t https://arxiv.org/abs/2503.12645\n",
      "\t https://ui.adsabs.harvard.edu/abs/2025arXiv250312645K/abstract\n",
      "An Optimal Algorithm for Strongly Convex Min-Min Optimization\n",
      "\t https://arxiv.org/abs/2212.14439\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv221214439G/abstract\n",
      "\t https://repository.kaust.edu.sa/handle/10754/686805\n",
      "\t https://openreview.net/forum?id=BdWKUqRQzx\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/4a253935-b92c-4230-8c25-5751f07df795/content\n",
      "Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?\n",
      "\t https://proceedings.mlr.press/v202/metelev23a.html\n",
      "\t https://openreview.net/forum?id=DwDQNKF4oy\n",
      "\t https://labmmo.ru/upload/000/u8/3/1/2301-11817.pdf\n",
      "\t https://arxiv.org/abs/2301.11817\n",
      "\t http://proceedings.mlr.press/v202/metelev23a/metelev23a.pdf\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3618408.3619429\n",
      "\t https://ui.adsabs.harvard.edu/abs/2023arXiv230111817M/abstract\n",
      "A hypothesis about the rate of global convergence for optimal methods (Newton’s type) in smooth convex optimization\n",
      "\t https://crm-en.ics.org.ru/journal/article/references/2685/\n",
      "\t https://crm-en.ics.org.ru/journal/article/2685/\n",
      "\t https://www.mathnet.ru/eng/crm253\n",
      "Decentralized saddle point problems via non-Euclidean mirror prox\n",
      "\t https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062\n",
      "\t https://repository.kaust.edu.sa/handle/10754/696834\n",
      "\t https://labmmo.ru/upload/000/u8/d/9/2102-07758v7.pdf\n",
      "\t https://repository.kaust.edu.sa/items/972f8f84-000d-409a-9a3e-fbae68b9bd4a\n",
      "Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems\n",
      "\t https://arxiv.org/abs/2307.12946\n",
      "\t https://ui.adsabs.harvard.edu/abs/2023arXiv230712946B/abstract\n",
      "\t https://openreview.net/forum?id=x1Ei4ETYx1\n",
      "Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters\n",
      "\t https://repository.kaust.edu.sa/handle/10754/678603\n",
      "\t https://elibrary.ru/download/elibrary_50045117_49345834.pdf#page=156\n",
      "\t https://repository.kaust.edu.sa/server/api/core/bitstreams/1b5dd049-1d24-44fb-86f0-6d5277f2ebc8/content\n",
      "\t https://link.springer.com/article/10.1007/s10287-023-00493-9\n",
      "\t https://labmmo.ru/upload/000/u8/6/4/2205-15669v3.pdf\n",
      "\t https://econpapers.repec.org/article/sprcomgts/v_3a21_3ay_3a2024_3ai_3a1_3ad_3a10.1007_5fs10287-023-00493-9.htm\n",
      "\t https://www.researchsquare.com/article/rs-3126039/latest\n",
      "\t https://search.proquest.com/openview/436fa63d2cb2020e61805817cc163230/1?pq-origsite=gscholar&cbl=54341\n",
      "\t https://arxiv.org/abs/2205.15669\n",
      "\t https://ideas.repec.org/a/spr/comgts/v21y2024i1d10.1007_s10287-023-00493-9.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2022arXiv220515669Y/abstract\n",
      "\t https://assets-eu.researchsquare.com/files/rs-3126039/v1_covered_1f055bfe-1abe-4181-a745-5b814687ba9c.pdf\n",
      "On Linear Convergence in Smooth Convex-Concave Bilinearly-Coupled Saddle-Point Optimization: Lower Bounds and Optimal Algorithms\n",
      "\t https://arxiv.org/abs/2411.14601\n",
      "\t https://ui.adsabs.harvard.edu/abs/2024arXiv241114601K/abstract\n",
      "\t https://openreview.net/forum?id=vVTgnjpaLp\n",
      "Decentralized finite-sum optimization over time-varying networks\n",
      "\t https://arxiv.org/abs/2402.02490\n",
      "\t https://openreview.net/forum?id=C5w86qtcgY\n",
      "\t https://labmmo.ru/upload/000/u8/e/e/2402-02490v3.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2024arXiv240202490M/abstract\n",
      "Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks\n",
      "\t https://proceedings.neurips.cc/paper_files/paper/2024/hash/af10f27d0a48175e486a647c06c7a9c6-Abstract-Conference.html\n",
      "\t https://ui.adsabs.harvard.edu/abs/2024arXiv240518031K/abstract\n",
      "\t https://labmmo.ru/upload/000/u8/3/5/2405-18031v1.pdf\n",
      "\t https://openreview.net/forum?id=IUKff7nYmW\n",
      "\t https://arxiv.org/abs/2405.18031\n",
      "\t https://dl.acm.org/doi/abs/10.5555/3737916.3740978\n",
      "SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration\n",
      "\t https://arxiv.org/abs/2506.23803\n",
      "\t https://ui.adsabs.harvard.edu/abs/2025arXiv250623803K/abstract\n",
      "On Solving Minimization and Min-Max Problems by First-Order Methods with Relative Error in Gradients\n",
      "\t https://arxiv.org/abs/2503.06628\n",
      "\t https://ui.adsabs.harvard.edu/abs/2025arXiv250306628V/abstract\n",
      "Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization\n",
      "\t https://arxiv.org/abs/2507.09823\n",
      "Decentralized Optimization with Coupled Constraints\n",
      "\t https://arxiv.org/abs/2407.02020\n",
      "\t https://openreview.net/forum?id=AJM52ygi6Y\n",
      "\t https://labmmo.ru/upload/000/u8/2/f/2407-02020v3.pdf\n",
      "\t https://ui.adsabs.harvard.edu/abs/2024arXiv240702020Y/abstract\n",
      "\t https://innopolis.university/filespublic/icomp/files/Demyan%20Yarmoshik.%20Decentralized%20Optimization%20with%20Coupled%20Constraints.pdf\n",
      "Convex-Concave Interpolation and Application of PEP to the Bilinear-Coupled Saddle Point Problem\n",
      "\t http://ndtest1.ics.org.ru/authors_nd/detail/473669-valery_krivchenko\n",
      "\t http://nd.ics.org.ru/article-10619/\n",
      "Optimal Algorithms for Affinely Constrained, Distributed, Decentralized, Minimax, and High-Order Optimization Problems\n",
      "\t https://repository.kaust.edu.sa/handle/10754/682331\n",
      "\t https://repository.kaust.edu.sa/bitstreams/115f71a4-0e9d-4dfe-9f09-a1434b75e39e/download\n"
     ]
    }
   ],
   "source": [
    "def split_by_year(\n",
    "    paper_list: typing.List[Paper], preprint: bool\n",
    ") -> typing.List[typing.Tuple[int, typing.List[Paper]]]:\n",
    "    paper_list.sort(reverse=preprint)\n",
    "    pd: typing.Dict[int, Paper] = {}\n",
    "    for paper in paper_list:\n",
    "        if paper.year in pd.keys():\n",
    "            pd[paper.year].append(paper)\n",
    "        else:\n",
    "            pd[paper.year] = [paper]\n",
    "    result = [(k, v) for k, v in pd.items()]\n",
    "    result.sort(reverse=True)\n",
    "    return result\n",
    "\n",
    "\n",
    "paper_list: typing.List[Paper] = []\n",
    "publication_list = []\n",
    "preprint_list = []\n",
    "for paper_dict in paper_dict_list:\n",
    "    paper = Paper(paper_dict)\n",
    "    if paper.type == \"publication\":\n",
    "        publication_list.append(paper)\n",
    "    elif paper.type == \"preprint\":\n",
    "        preprint_list.append(paper)\n",
    "\n",
    "publication_list = split_by_year(publication_list, False)\n",
    "preprint_list = split_by_year(preprint_list, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<span class=\"span-paper-title\">Decentralized Optimization with Coupled Constraints</span><span> (Demyan Yarmoshik, Alexander Rogozin, Nikita Kiselev, Daniil Dorin, Alexander Gasnikov, Dmitry Kovalev). International Conference on Learning Representations, 2025.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2407.02020\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2407.02020\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2407.02020\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2407.02020\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">On Linear Convergence in Smooth Convex-Concave Bilinearly-Coupled Saddle-Point Optimization: Lower Bounds and Optimal Algorithms</span><span> (Ekaterina Borodich, Dmitry Kovalev). International Conference on Machine Learning, 2025.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2411.14601\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2411.14601\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2411.14601\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2411.14601\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">An Optimal Algorithm for Strongly Convex Min-Min Optimization</span><span> (Dmitry Kovalev, Alexander Gasnikov, Grigory Malinovsky). Uncertainty in Artificial Intelligence, 2025.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2212.14439\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2212.14439\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2212.14439\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2212.14439\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2024/hash/af10f27d0a48175e486a647c06c7a9c6-Abstract-Conference.html\">Lower Bounds and Optimal Algorithms for Non-Smooth Convex Decentralized Optimization over Time-Varying Networks</a></span><span> (Dmitry Kovalev, Ekaterina Borodich, Alexander Gasnikov, Dmitrii Feoktistov). Advances in Neural Information Processing Systems, 2024.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2024/hash/af10f27d0a48175e486a647c06c7a9c6-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2405.18031\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2405.18031\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2024/hash/af10f27d0a48175e486a647c06c7a9c6-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2405.18031\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2405.18031\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/article/10.1007/s10287-023-00493-9\">Decentralized convex optimization on time-varying networks with application to Wasserstein barycenters</a></span><span> (Olga Yufereva, Michael Persiianov, Pavel Dvurechensky, Alexander Gasnikov, Dmitry Kovalev). Computational Management Science, 2024.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://link.springer.com/article/10.1007/s10287-023-00493-9\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2205.15669\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2205.15669\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://link.springer.com/article/10.1007/s10287-023-00493-9\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2205.15669\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2205.15669\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/article/10.1007/s10287-023-00485-9\">Decentralized saddle-point problems with different constants of strong convexity and strong concavity</a></span><span> (Dmitry Metelev, Alexander Rogozin, Alexander Gasnikov, Dmitry Kovalev). Computational Management Science, 2024.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://link.springer.com/article/10.1007/s10287-023-00485-9\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2206.00090\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2206.00090\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://link.springer.com/article/10.1007/s10287-023-00485-9\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2206.00090\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2206.00090\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062\">Decentralized saddle point problems via non-Euclidean mirror prox</a></span><span> (Alexander Rogozin, Aleksandr Beznosikov, Darina Dvinskikh, Dmitry Kovalev, Pavel Dvurechensky, Alexander Gasnikov). Optimization Methods and Software, 2024.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://labmmo.ru/upload/000/u8/d/9/2102-07758v7.pdf\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2023.2280062\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://labmmo.ru/upload/000/u8/d/9/2102-07758v7.pdf\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">Convex-Concave Interpolation and Application of PEP to the Bilinear-Coupled Saddle Point Problem</span><span> (Valery Krivchenko, Alexander Gasnikov, Dmitry Kovalev). Russian Journal of Nonlinear Dynamics, 2024.</span> []\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/article/10.1007/s10287-023-00479-7\">Non-smooth setting of stochastic decentralized convex optimization problem over time-varying graphs</a></span><span> (Aleksandr Lobanov, Andrew Veprikov, Georgiy Konin, Aleksandr Beznosikov, Alexander Gasnikov, Dmitry Kovalev). Computational Management Science, 2023.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://link.springer.com/article/10.1007/s10287-023-00479-7\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2307.00392\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2307.00392\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://link.springer.com/article/10.1007/s10287-023-00479-7\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2307.00392\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2307.00392\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/rwe/10.1007/978-3-030-54621-2_860-1\">Decentralized convex optimization over time-varying graphs</a></span><span> (Alexander Rogozin, Alexander Gasnikov, Aleksander Beznosikov, Dmitry Kovalev). Encyclopedia of Optimization, 2023.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://link.springer.com/rwe/10.1007/978-3-030-54621-2_860-1\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2210.09719\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2210.09719\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://link.springer.com/rwe/10.1007/978-3-030-54621-2_860-1\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2210.09719\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2210.09719\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://ems.press/journals/mag/articles/9939904\">Smooth monotone stochastic variational inequalities and saddle point problems: A survey</a></span><span> (Aleksandr Beznosikov, Boris Polyak, Eduard Gorbunov, Dmitry Kovalev, Alexander Gasnikov). European Mathematical Society Magazine, 2023.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://ems.press/journals/mag/articles/9939904\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2208.13592\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2208.13592\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://ems.press/journals/mag/articles/9939904\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2208.13592\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2208.13592\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v202/metelev23a.html\">Is consensus acceleration possible in decentralized optimization over slowly time-varying networks?</a></span><span> (Dmitry Metelev, Alexander Rogozin, Dmitry Kovalev, Alexander Gasnikov). International Conference on Machine Learning, 2023.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.mlr.press/v202/metelev23a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2301.11817\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2301.11817\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.mlr.press/v202/metelev23a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2301.11817\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2301.11817\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355\">Stochastic distributed learning with gradient quantization and double-variance reduction</a></span><span> (Samuel Horváth, Dmitry Kovalev, Konstantin Mishchenko, Peter Richtárik, Sebastian Stich). Optimization Methods and Software, 2023.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://core.ac.uk/download/pdf/599569336.pdf\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://www.tandfonline.com/doi/abs/10.1080/10556788.2022.2117355\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://core.ac.uk/download/pdf/599569336.pdf\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html\">Accelerated primal-dual gradient method for smooth and convex-concave saddle-point problems with bilinear coupling</a></span><span> (Dmitry Kovalev, Alexander Gasnikov, Peter Richtárik). Advances in Neural Information Processing Systems, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2112.15199\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2112.15199\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/883f66687a521536c505f9b2fbdcbf1e-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2112.15199\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2112.15199\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html\">Communication acceleration of local gradient methods via an accelerated primal-dual algorithm with an inexact prox</a></span><span> (Abdurakhmon Sadiev, Dmitry Kovalev, Peter Richtárik). Advances in Neural Information Processing Systems, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://papers.neurips.cc/paper_files/paper/2022/file/88c3c482430a62d35e03926a22e4b67e-Supplemental-Conference.pdf\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/88c3c482430a62d35e03926a22e4b67e-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://papers.neurips.cc/paper_files/paper/2022/file/88c3c482430a62d35e03926a22e4b67e-Supplemental-Conference.pdf\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html\">Optimal algorithms for decentralized stochastic variational inequalities</a></span><span> (Dmitry Kovalev, Aleksandr Beznosikov, Abdurakhmon Sadiev, Michael Persiianov, Peter Richtárik, Alexander Gasnikov). Advances in Neural Information Processing Systems, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2202.02771\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2202.02771\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/c959bb2cb164d37569a17fa67494d69a-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2202.02771\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2202.02771\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html\">Optimal gradient sliding and its application to optimal distributed optimization under similarity</a></span><span> (Dmitry Kovalev, Aleksandr Beznosikov, Ekaterina Borodich, Alexander Gasnikov, Gesualdo Scutari). Advances in Neural Information Processing Systems, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2205.15136\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2205.15136\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/d88f6f81e1aaf606776ffdd06fdf24ef-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2205.15136\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2205.15136\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html\">The first optimal acceleration of high-order methods in smooth convex optimization</a></span><span> (Dmitry Kovalev, Alexander Gasnikov). Advances in Neural Information Processing Systems, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2205.09647\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2205.09647\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/e56f394bbd4f0ec81393d767caa5a31b-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2205.09647\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2205.09647\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html\">The first optimal algorithm for smooth and strongly-convex-strongly-concave minimax optimization</a></span><span> (Dmitry Kovalev, Alexander Gasnikov). Advances in Neural Information Processing Systems, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/file/5e2ed801f62102f531d109d7c6e1b62f-Supplemental-Conference.pdf\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/hash/5e2ed801f62102f531d109d7c6e1b62f-Abstract-Conference.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2022/file/5e2ed801f62102f531d109d7c6e1b62f-Supplemental-Conference.pdf\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\">Accelerated variance-reduced methods for saddle-point problems</a></span><span> (Ekaterina Borodich, Vladislav Tominin, Yaroslav Tominin, Dmitry Kovalev, Alexander Gasnikov, Pavel Dvurechensky). EURO Journal on Computational Optimization, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2103.09344\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2103.09344\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"http://crm-en.ics.org.ru/uploads/crmissues/crm_2023_02/22_tominin.pdf\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2103.09344\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2103.09344\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v151/salim22a.html\">An optimal algorithm for strongly convex minimization under affine constraints</a></span><span> (Adil Salim, Laurent Condat, Dmitry Kovalev, Peter Richtárik). International Conference on Artificial Intelligence and Statistics, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.mlr.press/v151/salim22a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2102.11079\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2102.11079\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.mlr.press/v151/salim22a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2102.11079\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2102.11079\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">IntSGD: Adaptive floatless compression of stochastic gradients</span><span> (Konstantin Mishchenko, Bokun Wang, Dmitry Kovalev, Peter Richtárik). International Conference on Learning Representations, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2102.08374\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2102.08374\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2102.08374\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2102.08374\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html\">Lower bounds and optimal algorithms for smooth and strongly convex decentralized optimization over time-varying networks</a></span><span> (Dmitry Kovalev, Elnur Gasanov, Alexander Gasnikov, Peter Richtarik). Advances in Neural Information Processing Systems, 2021.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2106.04469\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2106.04469\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper/2021/hash/bc37e109d92bdc1ea71da6c919d54907-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2106.04469\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2106.04469\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v130/kovalev21a.html\">A linearly convergent algorithm for decentralized optimization: Sending less bits for free!</a></span><span> (Dmitry Kovalev, Anastasia Koloskova, Martin Jaggi, Peter Richtarik, Sebastian Stich). International Conference on Artificial Intelligence and Statistics, 2021.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.mlr.press/v130/kovalev21a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2011.01697\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2011.01697\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.mlr.press/v130/kovalev21a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2011.01697\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2011.01697\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"http://proceedings.mlr.press/v139/kovalev21a\">ADOM: accelerated decentralized optimization method for time-varying networks</a></span><span> (Dmitry Kovalev, Egor Shulgin, Peter Richtárik, Alexander V Rogozin, Alexander Gasnikov). International Conference on Machine Learning, 2021.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"http://proceedings.mlr.press/v139/kovalev21a\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2102.09234\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2102.09234\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"http://proceedings.mlr.press/v139/kovalev21a\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2102.09234\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2102.09234\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18\">Near-optimal decentralized algorithms for saddle point problems over time-varying networks</a></span><span> (Aleksandr Beznosikov, Alexander Rogozin, Dmitry Kovalev, Alexander Gasnikov). International Conference on Optimization and Applications, 2021.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2107.05957\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2107.05957\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_18\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2107.05957\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2107.05957\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19\">Towards accelerated rates for distributed optimization over time-varying networks</a></span><span> (Alexander Rogozin, Vladislav Lukoshkin, Alexander Gasnikov, Dmitry Kovalev, Egor Shulgin). International Conference on Optimization and Applications, 2021.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2009.11069\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2009.11069\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://link.springer.com/chapter/10.1007/978-3-030-91059-4_19\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2009.11069\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2009.11069\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\">Linearly converging error compensated SGD</a></span><span> (Eduard Gorbunov, Dmitry Kovalev, Dmitry Makarenko, Peter Richtárik). Advances in Neural Information Processing Systems, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2010.12292\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2010.12292\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/ef9280fbc5317f17d480e4d4f61b3751-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2010.12292\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2010.12292\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\">Optimal and practical algorithms for smooth and strongly convex decentralized optimization</a></span><span> (Dmitry Kovalev, Adil Salim, Peter Richtárik). Advances in Neural Information Processing Systems, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2006.11773\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2006.11773\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2020/hash/d530d454337fb09964237fecb4bea6ce-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2006.11773\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2006.11773\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v117/kovalev20a.html\">Don’t jump through hoops and remove those loops: SVRG and Katyusha are better without the outer loop</a></span><span> (Dmitry Kovalev, Samuel Horváth, Peter Richtárik). Algorithmic Learning Theory, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.mlr.press/v117/kovalev20a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/1901.08689\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/1901.08689\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.mlr.press/v117/kovalev20a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/1901.08689\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/1901.08689\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://link.springer.com/article/10.1134/S0965542520110020\">Accelerated methods for saddle-point problem</a></span><span> (Mohammad S Alkousa, Alexander Vladimirovich Gasnikov, Darina Mikhailovna Dvinskikh, Dmitry A Kovalev, Fedor Sergeevich Stonyakin). Computational Mathematics and Mathematical Physics, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://link.springer.com/article/10.1134/S0965542520110020\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/1906.03620\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/1906.03620\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://link.springer.com/article/10.1134/S0965542520110020\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/1906.03620\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/1906.03620\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"http://proceedings.mlr.press/v108/mishchenko20a\">Revisiting stochastic extragradient</a></span><span> (Konstantin Mishchenko, Dmitry Kovalev, Egor Shulgin, Peter Richtárik, Yura Malitsky). International Conference on Artificial Intelligence and Statistics, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"http://proceedings.mlr.press/v108/mishchenko20a\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/1905.11373\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/1905.11373\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"http://proceedings.mlr.press/v108/mishchenko20a\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/1905.11373\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/1905.11373\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v119/li20g.html\">Acceleration for compressed gradient descent in distributed and federated optimization</a></span><span> (Zhize Li, Dmitry Kovalev, Xun Qian, Peter Richtarik). International Conference on Machine Learning, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.mlr.press/v119/li20g.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2002.11364\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2002.11364\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.mlr.press/v119/li20g.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2002.11364\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2002.11364\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"http://proceedings.mlr.press/v119/malinovskiy20a.html\">From local SGD to local fixed-point methods for federated learning</a></span><span> (Grigory Malinovskiy, Dmitry Kovalev, Elnur Gasanov, Laurent Condat, Peter Richtarik). International Conference on Machine Learning, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"http://proceedings.mlr.press/v119/malinovskiy20a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2004.01442\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2004.01442\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"http://proceedings.mlr.press/v119/malinovskiy20a.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2004.01442\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2004.01442\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.mlr.press/v119/hanzely20b.html\">Variance reduced coordinate descent with acceleration: New method with a surprising application to finite-sum problems</a></span><span> (Filip Hanzely, Dmitry Kovalev, Peter Richtarik). International Conference on Machine Learning, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.mlr.press/v119/hanzely20b.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2002.04670\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2002.04670\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.mlr.press/v119/hanzely20b.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/2002.04670\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2002.04670\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\">RSN: randomized subspace Newton</a></span><span> (Robert Gower, Dmitry Kovalev, Felix Lieder, Peter Richtárik). Advances in Neural Information Processing Systems, 2019.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/1905.10874\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/1905.10874\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper/2019/hash/bc6dc48b743dc5d013b1abaebd2faed2-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/1905.10874\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/1905.10874\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\">Stochastic proximal Langevin algorithm: Potential splitting and nonasymptotic rates</a></span><span> (Adil Salim, Dmitry Kovalev, Peter Richtárik). Advances in Neural Information Processing Systems, 2019.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/1905.11768\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/1905.11768\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper/2019/hash/6a8018b3a00b69c008601b8becae392b-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/1905.11768\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/1905.11768\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\">Stochastic spectral and conjugate descent methods</a></span><span> (Dmitry Kovalev, Peter Richtarik, Eduard Gorbunov, Elnur Gasanov). Advances in Neural Information Processing Systems, 2018.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/1802.03703\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/1802.03703\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://proceedings.neurips.cc/paper_files/paper/2018/hash/e721a54a8cf18c8543d44782d9ef681f-Abstract.html\">Link<i class=\"fa fa-solid fa-link\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/abs/1802.03703\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/1802.03703\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\"><a href=\"https://crm-en.ics.org.ru/journal/article/references/2685/\">A hypothesis about the rate of global convergence for optimal methods (Newton’s type) in smooth convex optimization</a></span><span> (Alexander Gasnikov, Dmitry Kovalev). Computer research and modeling, 2018.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://crm-en.ics.org.ru/journal/article/references/2685/\">Link<i class=\"fa fa-solid fa-link\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://crm-en.ics.org.ru/journal/article/references/2685/\">Link<i class=\"fa fa-solid fa-link\"></i></a>]\n",
      "<span class=\"span-paper-title\">Nesterov Finds GRAAL: Optimal and Adaptive Gradient Method for Convex Optimization</span><span> (Ekaterina Borodich, Dmitry Kovalev). arXiv preprint arXiv:2507.09823, 2025.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2507.09823\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2507.09823\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2507.09823\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2507.09823\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">SGD with Adaptive Preconditioning: Unified Analysis and Momentum Acceleration</span><span> (Dmitry Kovalev). arXiv preprint arXiv:2506.23803, 2025.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2506.23803\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2506.23803\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2506.23803\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2506.23803\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization</span><span> (Dmitry Kovalev). arXiv preprint arXiv:2503.12645, 2025.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2503.12645\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2503.12645\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2503.12645\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2503.12645\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">On Solving Minimization and Min-Max Problems by First-Order Methods with Relative Error in Gradients</span><span> (Artem Vasin, Valery Krivchenko, Dmitry Kovalev, Fedyor Stonyakin, Nazari Tupitsa, Pavel Dvurechensky, Mohammad Alkousa, Nikita Kornilov, Alexander Gasnikov). arXiv preprint arXiv:2503.06628, 2025.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2503.06628\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2503.06628\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2503.06628\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2503.06628\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">Decentralized finite-sum optimization over time-varying networks</span><span> (Dmitry Metelev, Savelii Chezhegov, Alexander Rogozin, Aleksandr Beznosikov, Alexander Sholokhov, Alexander Gasnikov, Dmitry Kovalev). arXiv preprint arXiv:2402.02490, 2024.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2402.02490\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2402.02490\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2402.02490\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2402.02490\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">Optimal algorithm with complexity separation for strongly convex-strongly concave composite saddle point problems</span><span> (Ekaterina Borodich, Georgiy Kormakov, Dmitry Kovalev, Aleksandr Beznosikov, Alexander Gasnikov). arXiv preprint arXiv:2307.12946, 2023.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2307.12946\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2307.12946\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2307.12946\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2307.12946\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">On scaled methods for saddle point problems</span><span> (Aleksandr Beznosikov, Aibek Alanov, Dmitry Kovalev, Martin Takáč, Alexander Gasnikov). arXiv preprint arXiv:2206.08303, 2022.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2206.08303\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2206.08303\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2206.08303\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2206.08303\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">Decentralized distributed optimization for saddle point problems</span><span> (Alexander Rogozin, Aleksandr Beznosikov, Darina Dvinskikh, Dmitry Kovalev, Pavel Dvurechensky, Alexander Gasnikov). arXiv preprint arXiv:2102.07758, 2021.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2102.07758\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2102.07758\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2102.07758\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2102.07758\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">Fast linear convergence of randomized BFGS</span><span> (Dmitry Kovalev, Robert M Gower, Peter Richtárik, Alexander Rogozin). arXiv preprint arXiv:2002.11337, 2020.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/2002.11337\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/2002.11337\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/2002.11337\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/2002.11337\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">Distributed fixed point methods with compressed iterates</span><span> (Sélim Chraibi, Ahmed Khaled, Dmitry Kovalev, Peter Richtárik, Adil Salim, Martin Takáč). arXiv preprint arXiv:1912.09925, 2019.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/1912.09925\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/1912.09925\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/1912.09925\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/1912.09925\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n",
      "<span class=\"span-paper-title\">Stochastic Newton and cubic Newton methods with simple local linear-quadratic rates</span><span> (Dmitry Kovalev, Konstantin Mishchenko, Peter Richtárik). arXiv preprint arXiv:1912.01597, 2019.<br/><div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/abs/1912.01597\">arXiv<i class=\"ai ai-arxiv\"></i></a></div> <div style=\"display:inline-block\"><a class=\"paper-link\" href=\"https://arxiv.org/pdf/1912.01597\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a></div></span> [<a class=\"paper-link\" href=\"https://arxiv.org/abs/1912.01597\">arXiv<i class=\"ai ai-arxiv\"></i></a>, <a class=\"paper-link\" href=\"https://arxiv.org/pdf/1912.01597\">PDF<i class=\"fa fa-regular fa-file-pdf\"></i></a>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94717"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_loader = jinja2.FileSystemLoader(\"\")\n",
    "env = jinja2.Environment(loader=file_loader)\n",
    "template = env.get_template(\"template_index.html\")\n",
    "\n",
    "output = template.render(\n",
    "    publications=publication_list,\n",
    "    preprints=preprint_list,\n",
    ")\n",
    "open(\"index.html\", \"w\").write(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Render CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CV2/date.tex\", \"w\") as file:\n",
    "    file.write(\"\\\\begin{center}\\n\")\n",
    "    file.write(\n",
    "        \"Last Updated on {}\\n\".format(datetime.datetime.now().strftime(\"%B %d, %Y\"))\n",
    "    )\n",
    "    file.write(\"\\\\end{center}\\n\")\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CV2/papers.tex\", \"w\") as file:\n",
    "    file.write(\"\\\\section{Publications}\\n\")\n",
    "    file.write(\"\\\\begin{enumerate}\\n\")\n",
    "\n",
    "    for year, paper_list in publication_list:\n",
    "        for paper in paper_list:\n",
    "            file.write(\"\\\\item\\n\")\n",
    "            file.write(f\"{paper.to_latex()}\\n\")\n",
    "\n",
    "    file.write(\"\\\\end{enumerate}\\n\")\n",
    "\n",
    "    file.write(\"\\\\section{Preprints}\\n\")\n",
    "    file.write(\"\\\\begin{enumerate}\\n\")\n",
    "\n",
    "    for year, paper_list in preprint_list:\n",
    "        for paper in paper_list:\n",
    "            file.write(\"\\\\item\\n\")\n",
    "            file.write(f\"{paper.to_latex()}\\n\")\n",
    "\n",
    "    file.write(\"\\\\end{enumerate}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "website",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
